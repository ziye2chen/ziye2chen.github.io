---
layout: distill
title: Introduction to KAN
date: 2024-06-01 00:01:00
description: this is an introduction about KAN
tags: structure 
categories: sample-posts
tabs: true
featured: true

toc:
  - name: KAN and MLP
  - name: Review of MLP
    subsections:
      - name: The structure of MLP
      - name: The shortcomings of MLP
    
    # if a section has subsections, you can add them as follows:
  - name: KAN
    subsections:
      - name: Why KAN is awesome

_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

## KAN and MLP

The emergence of the KAN (Kolmogorov-Arnold Network) model has led to heated discussions in the AI community, and the reason for this is that the flaws in the structure of the MLP itself have been troubling the researchers involved. 

Nowadays, most AI research is based on MLP. The appearance of KAN can potentially revolutionize the direction of AI research. If KAN is as marvelous as expected in subsequent research, its appearance will be like the emergence of cement in a society that has been only using wood to build houses.


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/KAN/2_1.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

## Review of MLP

The theoretical basis for MLP is that a single hidden layer network containing enough neurons can approximate any continuous function. But MLP has some natural flaws:

#### Vanishing and exploding gradients

**Vanishing gradients:** the gradient tends to zero, the network weights cannot be updated or are updated very slightly, and the network will not be effective even if it is trained for a long time;

**Exploding gradient:** the gradient grows exponentially and becomes very large, leading to a large update of the network weights, making the network unstable.

Whether the gradient disappears or the gradient explosion is essentially due to the backpropagation of the deep neural network.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/KAN/NN.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

$$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial y_4} \frac{\partial y_4}{\partial z_4} \frac{\partial z_4}{\partial y_3} \frac{\partial y_3}{\partial z_3} \frac{\partial z_3}{\partial y_2} \frac{\partial y_2}{\partial z_2} \frac{\partial z_2}{\partial y_1} \frac{\partial y_1}{\partial z_1} \frac{\partial z_1}{\partial w_1}$$
