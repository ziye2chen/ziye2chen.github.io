---
layout: distill
title: Introduction to KAN
date: 2024-06-01 00:01:00
description: this is an introduction about KAN
tags: structure 
categories: sample-posts
tabs: true
featured: true

toc:
  - name: KAN and MLP
  - name: The drawbacks of MLP  
    # if a section has subsections, you can add them as follows:
  - name: KAN
    subsections:
      - name: Why KAN is awesome

_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

## KAN and MLP

The emergence of the KANs (Kolmogorov-Arnold Networks) model has led to heated discussions in the AI community, and the reason for this is that the flaws in the structure of the MLPs itself have been troubling the researchers involved. 

Nowadays, most AI research is based on MLP. The appearance of KANs can potentially revolutionize the direction of AI research. If KANs are as marvelous as expected in subsequent research, their appearance will be like the emergence of cement in a society that has been only using wood to build houses.


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/KAN/2_1.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

## The drawbacks of MLP

The theoretical basis for MLPs is that a single hidden layer network containing enough neurons can approximate any continuous function. But MLPs have some natural flaws:

### Vanishing and exploding gradients

***Vanishing gradients:*** the gradient tends to zero, the network weights cannot be updated or are updated very slightly, and the network will not be effective even if it is trained for a long time;

***Exploding gradient:*** the gradient grows exponentially and becomes very large, leading to a large update of the network weights, making the network unstable.

Whether the gradient disappears or the gradient explosion is essentially due to the backpropagation of the deep neural network.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/KAN/NN.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

For example, in a four-layer neural network, its backpropagation formula is as follows:

$$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial y_4} \frac{\partial y_4}{\partial z_4} \frac{\partial z_4}{\partial y_3} \frac{\partial y_3}{\partial z_3} \frac{\partial z_3}{\partial y_2} \frac{\partial y_2}{\partial z_2} \frac{\partial z_2}{\partial y_1} \frac{\partial y_1}{\partial z_1} \frac{\partial z_1}{\partial w_1}$$

$$\frac{\partial L}{\partial y_4} = \sigma'(z_4) w_4 \sigma'(z_3) w_3 \sigma'(z_2) w_2 \sigma'(z_1) x_1$$

where $$y_i = \sigma(z_i) = \sigma(w_i x_i + b_i)$$, which $$x_i = y_{i-1}$$, and $$\sigma$$ is ***sigmoid*** activation function.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/KAN/sigmoid.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

From the above, it can be seen that there is a sequence of multiplications in the backpropagation of the neural network. Also, the derivative of the sigmoid activation function has a value range of 0 to 0.25. So when each $$w_i$$ is small, and there are many $$\sigma'$$ multiplied together, the gradient tends to 0. When each $$w_i$$ very large, the gradient becomes very large.

While many methods say they solve this problem, the issue is still complicated to avoid completely, as it is an intrinsic shortcoming of MLPs.

### Inefficient utilization of parameters

MLPs usually use fully connected networks, so the number of parameters is huge. When the network becomes very deep, only a few parameters are utilized. And this is one reason why LLM may have reached a dead end.

Although it is possible to simplify the structure using CNNs or regularization or the like, essentially, the structure of a highly dense continuous linear model coupled with an activation function dictates that the MLP can't be too sparse or will not have enough representational power.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/KAN/cnn.jpeg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

### Inadequate capacity to handle high-dimensional data and Long-term dependency issues

MLPs do not utilize the intrinsic structure of the data, such as local spatial correlations in images and sequence information in textual data. Although CNN and RNN can improve these problems, this base module of MLPs determines that these problems cannot be solved completely.

MLPs also have difficulty capturing long-standing relationships in input sequences. (RNN & Transformer can improve it)

## Kolmogorov-Arnold Networks

### The Kolmogorov-Arnold representation theorem

KANs are inspired by the Kolmogorov-Arnold representation theorem. Vladimir Arnold and Andrey Kolmogorov established that if f is a multivariate continuous function
on a bounded domain, then f can be written as a finite composition of continuous functions of a single variable and the binary operation of addition. 

More specifically, for a smooth $$f : [0, 1]^n \rightarrow \mathbb{R}$$,

$$f(\mathbf{x}) = f(x_1, \dots, x_n) = \sum_{q=1}^{2n+1} \Phi_q \left( \sum_{p=1}^n \phi_{q,p}(x_p) \right)$$

where $$\phi_{q,p} : [0,1] \rightarrow \mathbb{R}$$ and $$\Phi_q : \mathbb{R} \rightarrow \mathbb{R}$$. This theorem reveals how any multivariate continuous function can be represented by a simpler set of functions.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/KAN/2_2.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

It can be shown that the Kolmogorov-Arnold representation theorem is a simple KANs which has only two-layer nonlinearities and a small number of terms (2n + 1) in the hidden layer. This structure appeared a long time ago, but the difficulty is how to make the KANs deeper. And two-layer networks are too simple to approximate complex functions. (Details of deepening KANs will be mentioned later)

### Why KANs are awesome

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/KAN/0_1.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

KANs contrast MLPs by not having a nested relationship between activation functions and parameter matrices but rather a nesting of directly nonlinear functions $$\Phi$$. And $$\Phi$$ all use the same function structure. (In the paper, they use the spline spline function in numerical analysis)

$$KAN(x) = (\Phi_{L-1} \circ \Phi_{L-2} \circ \cdots \circ \Phi_1 \circ \Phi_0)x$$

$$MLP(x) = (W_{L-1} \circ \sigma \circ W_{L-2} \circ \sigma \circ \cdots \circ W_1 \circ \sigma \circ W_0)x$$


When learning the network parameters, MLPs learn the linear function with a fixed nonlinear activation function. In contrast, KANs learn the parameterized nonlinear function directly, which enhances their characterization ability.

Because of the parameters' complexity, although individual spline functions are more complex to learn than linear functions, KANs typically allow for smaller computational graphs than MLPs (i.e., a smaller network size is required to achieve the same effect).

### The details about KANs




