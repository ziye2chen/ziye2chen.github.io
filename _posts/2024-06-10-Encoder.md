---
layout: distill
title: Encoder
date: 2099-06-10 00:01:00
description: this is an introduction about KAN
tags: structure paperReading
categories: AI
tabs: true
featured: true
disqus_comments: true

toc:
  - name: Token Embedding
  - name: Position Embedding
    # if a section has subsections, you can add them as follows:
  - name: Kolmogorov-Arnold Networks
    subsections:
      - name: The Kolmogorov-Arnold representation theorem
      - name: Why KAN is awesome
      - name: The details about KANs


---

## Token Embedding

<d-code block language="python">
class TokenEmbedding(nn.Embedding):
    def __init__(self, vocab_size, d_model):
        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)
</d-code>

## Position Embedding

Let $$ t $$ be the desired position in an input sentence, $$ \vec{p}_t \in \mathbb{R}^d $$ be its corresponding encoding, and $$ d $$ be the encoding dimension (where $$ d \geq 0 $$). Then $$ f : \mathbb{N} \rightarrow \mathbb{R}^d $$ will be the function that produces the output vector $$ \vec{p}_t $$ and it is defined as follows:

$$
\vec{p}_t^{(i)} = f(t)^{(i)} := 
\begin{cases} 
\sin(\omega_i \cdot t), & \text{if } i = 2k \\
\cos(\omega_i \cdot t), & \text{if } i = 2k + 1
\end{cases}
$$

where

$$
\omega_i = \frac{1}{10000^{2i/d}}
$$

$$
PE_{\text{pos},2i} = \sin\left(\frac{\text{pos}}{10000^{2i/d}}\right)
$$
$$
PE_{\text{pos},2i+1} = \cos\left(\frac{\text{pos}}{10000^{2i/d}}\right)
$$

<d-code block language="python">
class PositionalEmbedding(nn.Module):
    def __init__(self, d_model, maxlen, device):
        super(PositionalEmbedding, self).__init__()
        self.encoding = torch.zeros(maxlen, d_model, device=device) # initialize the encoding
        self.encoding.requires_grad_(False) # This encoding does not require a gradient

        # generate the position (The most important one in positional embedding!!!):
        pos = torch.arange(0, maxlen, device=device) # generate a series from 0 to maxlen-1 [0, 1, ... , maxlen-1]
        pos = pos.float().unsqueeze(1) # add a dimension [[0.], [1.], ... , [maxlen-1]]
        _2i = torch.arange(0, d_model, 2, device=device) # generate 2i: [0, 2, 4, ...]

        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))
        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))

    def forward(self, x):
        # there is no need to RETURN all the arguments, so make a cut with seq_len
        seq_len = x.shape[1]
        return self.encoding[:seq_len, :]
</d-code>

## Layer Norm
<d-code block language="python">
class LayerNorm(nn.Module):
    def __init__(self, d_model, eps=1e-10):
        super(LayerNorm, self).__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.zeros(d_model))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        var = x.var(-1, unbiased=False, keepdim=True)
        out = (x - mean) / torch.sqrt(var + self.eps)
        out = self.gamma * out + self.beta # Finally, the result is scaled and offset.
        return out
</d-code>


<d-code block language="python">
</d-code>
