---
layout: distill
title: Modern Solution of Optimal Transport - Sinkhorn
date: 2024-03-23 00:01:00
description: this is my study note about sinkhorn
tags: math
categories: sample-posts
tabs: true
featured: true

toc:
  - name: Modern Solution of Optimal Transport - Sinkhorn
    subsections:
      - name: Sinkhorn Distance
      - name: Computing Regularized Transport with Sinkhorn’s Algorithm
      - name: PyTorch implementation of sinkhorn
    


_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

## Modern Solution of Optimal Transport - Sinkhorn

In 2013, Cuturi proposed a scalable approximation of optimal transport. Since then, OT is increasingly becoming a core tool of machine learning research toolbox.

The computation of other optimal transport formulation involves the resolution of a linear program whose cost can quickly become prohibitive.

But Sinkhorn distance looks at transport problems from a maximum-entropy perspective. It smooths the classic optimal transport problem with an entropic regularization term. 
And the computation through Sinkhorn’s matrix scaling algorithm is several orders of magnitude faster than that of transport solvers, which is $$O(n^2)$$.

### Sinkhorn Distance

In what follows, $$\left \langle \cdot , \cdot   \right \rangle $$ stands for the Frobenius dot-product. For two probability vectors $$\mu$$ and $$\nu$$ in the simplex $$\sum_d := \{ x \in \mathbb{R}_+^{d} : x^T \mathbf{1}_d=1 \}$$, where $$\mathbf{1}_d$$ is the d dimensional vector of ones, we write $$U(r,c)$$ for the transport polytope of $$\mu$$ and $$\nu$$, namely the polyhedral set of $$d \times d$$ matrices,

$$ U(\mu,\nu) := \{ P \in \mathbb{R}^{d\times d}_+ | P \mathbf{1}_d = \mu, P^T \mathbf{1}_d = \nu \} $$

And we define the entropy $h$ and Kullback-Leibler divergences of $$P,Q\in U(\mu,\nu)$$ and a marginals $$\mu \in \sum_d$$ as

$$ h(\mu) = -\sum_{i=1}^{d}\mu_i log \mu_i,\quad h(P) = -\sum_{i,j=1}^{d}p_{ij} log p_{ij}, \quad \mathbf{KL}(P||Q)=\sum_{ij}p_{ij} log\frac{p_{ij}}{q_{ij}} $$

where $$p(X=i,Y=j)=p_{ij}$$ and the same as $$q_ij$$.

The following information theoretic inequality (Cover and Thomas, 1991, §2) for joint probabilities

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/optimalTransport/equation/3_1.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

is tight, since the independence table $$\mu\nu^T$$ has entropy $$h(\mu\nu^T) = h(\mu)+h(\nu)$$. By the concavity of entropy, we can introduce the convex set

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/optimalTransport/equation/3_2_3_3.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

These two definitions are indeed equivalent, since one can easily check that $$\mathbf{KL}(P||\mu\nu^T)= h(\mu)+h(\nu)-h(P)$$, a quantity which is also the mutual information $I(X||Y)$ of two random variables $$(X,Y)$$ should they follow the joint probability $$P$$ (Cover and Thomas, 1991, §2).

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/optimalTransport/equation/3_4.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

Why consider an entropic constraint in optimal transport? The first reason is computational. The second reason is built upon the following intuition. As a classic result of linear optimization, the OT problem is always solved on a vertex of $$U(\mu,\nu)$$. Such a vertex is a sparse $$d\times d$$ matrix with only up to $$2d-1$$ non-zero elements (Brualdi, 2006, §8.1.3).

When α is large enough, the Sinkhorn distance coincides with the classic OT distance. When α = 0, the Sinkhorn distance has a closed form and becomes a negative definite kernel if one assumes that M is itself a negative definite distance, or equivalently a Euclidean distance matrix.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/optimalTransport/equation/3_5.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/optimalTransport/equation/3_6.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/optimalTransport/equation/3_7.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

And $$d_{M,\alpha}(x,y)$$ also satisfies the triangle inequality:

$$ d_{M,\alpha}(x,z) \le d_{M,\alpha}(x,y)+d_{M,\alpha}(y,z)$$

### Computing Regularized Transport with Sinkhorn’s Algorithm
We consider in this section a Lagrange multiplier for the entropy constraint of Sinkhorn distances:

$$For \lambda > 0, d_{M}^{\lambda}(\mu, \nu) := \langle P^{\lambda}, M \rangle, \text{where} \ P^{\lambda} = \underset{P \in U(\mu,\nu)}{\text{argmin}} \langle P, M \rangle - \frac{1}{\lambda}h(P).$$

By duality theory we have that to each $$\alpha$$ corresponds a $$\lambda \in [0,\infty]$$ such that $$d_{M,\alpha}(\mu, \nu)=d_{M}^{\lambda}(\mu, \nu)$$ holds for that pair $$(\mu,\nu)$$. We call $$d_{M}^{\lambda}$$ the dual-Sinkhorn divergence and show that it can be computed for a much cheaper cost than the original distance $$d_M$$.

\begin{algorithm} 
\caption{computation of $$\mathbf{d} = [d_M^\lambda(\mu,\nu_1),\cdots,d_M^\lambda(\mu,\nu_N)]$$, using Matlab syntax} 
\begin{algorithmic}
\REQUIRE $$M,\lambda, \mu, \nu:=[\nu_1,\cdots,\nu_N]$$
    \STATE $$I=(\mu >0)$$; $$\mu=\mu(I)$$; $$M=M(I,:)$$; $$K=exp(-\lambda M)$$
    \STATE $$u=$$ones(length($$\mu$$),N)/length($$\mu$$);
    \STATE $$\widetilde{K}=$bsxfun(@rdivide,K,$\mu$$)\% equivalent to $$\widetilde{K}=$$ $$\mathbf{diag}(1./\mu$$)K
    \WHILE{$$u$$ changes or any other relevant stopping criterion}
    \STATE $$u=1./(\widetilde{K}(\nu./(K'u)))$$
    \ENDWHILE
    \STATE $$v=\nu./(K'u)$$
    \STATE $$\mathbf{d}=\mathbf{sum}(u.*((K.*M)v)$$
\end{algorithmic} 
\end{algorithm}

### PyTorch implementation of sinkhorn