<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ziye2chen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ziye2chen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-12T22:07:33+00:00</updated><id>https://ziye2chen.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The detail about Attention structure</title><link href="https://ziye2chen.github.io/blog/2024/attention/" rel="alternate" type="text/html" title="The detail about Attention structure"/><published>2024-05-23T00:01:00+00:00</published><updated>2024-05-23T00:01:00+00:00</updated><id>https://ziye2chen.github.io/blog/2024/attention</id><content type="html" xml:base="https://ziye2chen.github.io/blog/2024/attention/"><![CDATA[<h2 id="attention-structure">Attention Structure</h2> <h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3> <p>The attention architecture derives from the mechanisms of human attention. When an image is placed in front of a human, the human scans the global image to obtain areas that are worth focusing on, and devotes more attention to these areas to obtain information. Nowadays, the popular attention model generally relies on the encoder-decoder framework, which can deal with tasks including NLP, image processing, etc.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/scaledAndMultiAttention-480.webp 480w,/assets/img/attention/scaledAndMultiAttention-800.webp 800w,/assets/img/attention/scaledAndMultiAttention-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/scaledAndMultiAttention.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Scaled Dot-Product Attention is the most basic attention structure. Multi-Head Attention is multiple parallel Scaled Dot-Product Attention, which splices the output of each Scaled Dot-Product Attention and does a linear transformation to output.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/transform-480.webp 480w,/assets/img/attention/transform-800.webp 800w,/assets/img/attention/transform-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/transform.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Attention can be considered as a specific word weighting, given a sequence of inputs \(\{x_1, x_2, \dots, x_n\}\), after the attentions layer, combining the input individuals \(\{x_1, x_2, \dots, x_n\}\) and outputs \(\{y_1, y_2, \dots, y_n\}\) will be as follows:</p> <ol> <li>The input features are multiplied by three sets of weight matrices \(W^Q, W^K, W^V\) to generate the three matrices of query, key, and value;</li> <li>The attention weight matrix \(A\) is obtained from the product of a certain \(Q\) and \(K\), which is normalized to get \(\hat{A}\);</li> <li>The normalized weights \(\hat{A}\) are multiplied by V to get the final output feature \(O\).</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/qkv-480.webp 480w,/assets/img/attention/qkv-800.webp 800w,/assets/img/attention/qkv-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/qkv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Mathematically, the self-attention matrix for input matrices \((Q, K, V)\) is calculated as:</p> \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/Y-480.webp 480w,/assets/img/attention/Y-800.webp 800w,/assets/img/attention/Y-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/Y.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="multi-head-attention">Multi-Head Attention</h3> <p>If each vector \(q_i, k_i, v_i\) is split into n, we can get the n-headed attention mechanism. It is recognized that multi-head self-attention mechanisms are better than single-head ones because it can capture information in more dimensions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/multihead-480.webp 480w,/assets/img/attention/multihead-800.webp 800w,/assets/img/attention/multihead-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/multihead.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>At the beginning, we need to import some libraries we will normally use.</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">import torch
from torch import nn
import torch.functional as F
import math
</span></code></pre></div></div> <p>We need some data for testing. The dimension of testing data is 3.</p> <ul> <li>The first dimension represents batch.</li> <li>The second dimension represents time.</li> <li>The third dimension represnts the dimension after Encoder.</li> </ul> <p>And in language model, the ‘512’ below normally means the dimensions of the word vector do you want to map your word to after Embedding.</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">X = torch.randn(128,64,512) #Batch, Time, Dimension
print(X.shape)
</span></code></pre></div></div> <p>Next, we set up the basic parameters of Multihead attention.</p> <ul> <li>d_model represents the number of dimensions I want to map to the QKV space.</li> <li>n_head repersents the number of head.</li> </ul> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">d_model = 512
n_head = 8
</span></code></pre></div></div> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">class multi_head_attention(nn.Module):
</span># When we are writing a pytorch class, we need to inherit nn.Module
    def __init__(self, d_model, n_head) -&gt; None:
        super(multi_head_attention, self).__init__() # Initialize some basic parameters
<span class="err">
</span>        self.d_model = d_model
        self.n_head = n_head
<span class="err">
</span>        self.w_q = nn.Linear(d_model, d_model) #Query
        self.w_k = nn.Linear(d_model, d_model) #Key
        self.w_v = nn.Linear(d_model, d_model) #Value
        # It's like we are looking for some queries to match some keys, and values are then weighted to combine.
<span class="err">
</span>        self.w_combine = nn.Linear(d_model, d_model)
        # Because of 'multi-head', we need a combinatorial mapping of the outputs.
<span class="err">
</span>        self.softmax = nn.Softmax(dim=-1)
</code></pre></div></div> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#Still in class multi_head_attention(nn.Module):
    def forward(self, q, k, v, mask = None):
        batch, time, dimension = q.shape # get the dimensions of Q, K, V
        n_d = self.d_model // self.n_head # Because we have n heads, 'd_model' for each submodel has to be divisible by 'n_head'.
        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)
<span class="err">
</span>        q = q.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)   
        k = k.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)   
        v = v.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)   
        # We cannot put 'n_head' in the last dimension because we need to process the last two dimensions.
<span class="err">
</span>        score = q @ k.transpose(2, 3) / math.sqrt(n_d) # The most important code in Attention.
<span class="err">
</span>        minusInfty = -10000
        if mask is not None:
            score = score.masked_fill(mask == 0, minusInfty)
        score = self.softmax(score) @ v
<span class="err">
</span>        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension)
<span class="err">
</span>        output = self.w_combine(score)
        
        return output 
</code></pre></div></div> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">attention = multi_head_attention(d_model, n_head)
output = attention(X, X, X)
print(output, output.shape)
</span></code></pre></div></div> <h2 id="something-you-might-want-to-know-about-attention">Something you might want to know about Attention</h2> <h3 id="the-square-root-of-d-in-the-attention-calculation">The square root of d in the Attention calculation</h3> <p><strong>Why should we divide \(\sqrt{d}\) in the Attention calculation?</strong></p> <p>Assume that the Query vector \(q\) and the Key vector \(k\) have dimension \(d\) and their elements are independent and identically distributed random variables with mean \(0\) and variance \(1\).</p> <p>Then their dot product \(q \cdot k\) can be expressed as: \(q \cdot k = \sum_{i=1}^d q_i k_i\). Since each \(q_i\) and \(k_i\) is an independently and identically distributed random variable, their product \(q_i k_i\) is also a random variable. For each \(q_i k_i\), the expectation is \(0\) and the variance is \(1\).</p> <p>Therefore, the expected value of the dot product \(q \cdot k\) is:</p> \[\mathbb{E}[q \cdot k] = \mathbb{E} \left[ \sum_{i=1}^d q_i k_i \right] = \sum_{i=1}^d \mathbb{E}[q_i k_i] = 0\] <p>Moreover, its variance is:</p> \[\text{Var}(q \cdot k) = \text{Var} \left( \sum_{i=1}^d q_i k_i \right) = \sum_{i=1}^d \text{Var}(q_i k_i) = d\] <p>Thus, the standard deviation of the dot product \(q \cdot k\) is:</p> \[\sigma = \sqrt{\text{Var}(q \cdot k)} = \sqrt{d}\] <p>By the Central Limit Theorem, the sum of these independently and identically distributed random variables will converge to a normal distribution when $d$ is large enough. To normalize this, we subtract the expected value and divide by the standard deviation:</p> \[\frac{q \cdot k - \mathbb{E}[q \cdot k]}{\sqrt{\text{Var}(q \cdot k)}}\] <p>Here, \(\mathbb{E}[q \cdot k] = 0\) and \(\text{Var}(q \cdot k) = d\), so the expression becomes:</p> \[\frac{q \cdot k}{\sqrt{d}}\] <p>This will make the expression approximately obey the standard normal distribution \(N(0, 1)\).</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="structure"/><category term="code"/><summary type="html"><![CDATA[this is the detail about attention structure]]></summary></entry><entry><title type="html">Optimal Transport</title><link href="https://ziye2chen.github.io/blog/2024/optimal-transport/" rel="alternate" type="text/html" title="Optimal Transport"/><published>2024-03-23T00:01:00+00:00</published><updated>2024-03-23T00:01:00+00:00</updated><id>https://ziye2chen.github.io/blog/2024/optimal-transport</id><content type="html" xml:base="https://ziye2chen.github.io/blog/2024/optimal-transport/"><![CDATA[<h2 id="intro-to-optimal-transport">Intro to Optimal Transport</h2> <p>Optimal transport problem can be understood as a problem of two piles of soil, shoveled from soil A to the other and eventually piled up to form soil B. And your mission is to find the most efficient method to shovel the soil.</p> <p>There are two ways to formulate the optimal transport problem: the Monge and Kantorovich formulations. Historically the Monge formulation comes before Kantorovich. The Kantorovich formulation can be seen as a generalisation of Monge. Both formulations have their advantages and disadvantages. Monge is more useful in applications, whilst Kantorovich is more useful theoretically.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/soil-480.webp 480w,/assets/img/optimalTransport/soil-800.webp 800w,/assets/img/optimalTransport/soil-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/soil.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="measure">Measure</h3> <p>In mathematics, measure is a generalization and formalization of geometrical measures like length, area, volume, magnitude, mass and probability. In Lebesgue measure, for lower dimensions n = 1, 2, or 3, it coincides with the standard measure of length, area, or volume.</p> <p>The concept of measures will be used later, and before I introduce probability measures, I need to introduce the histogram. The histogram here is the probability of n sums of 1, representing a probability distribution. We will use interchangeably the terms histogram and probability vector for any element $a \in \Sigma_n $ that belongs to the probability simplex</p> \[\Sigma_n := \{a \in \mathbb{R}_{+}^n : \sum_{i=1}^n a_i = 1 \}\] <p>And we use discrete measure describes a probability measure if, additionally, \(a \in \Sigma_n\) and more generally a positive measure if all the elements of vector \(a\) are non-negative.</p> <p>A discrete measure with weights \(a\) and locations \(x_1, \cdots, x_n \in \mathcal{X}\) reads \begin{equation} \label{inter} \alpha = \sum_{i=1}^n a_i \delta_{x_i}, \end{equation} where \(\delta_x\) is the Dirac at position x, intuitively a unit of mass which is infinitely concentrated at location \(x\).</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="math"/><summary type="html"><![CDATA[this is an introduction about Optimal Transport]]></summary></entry></feed>