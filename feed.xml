<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ziye2chen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ziye2chen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-06T01:02:34+00:00</updated><id>https://ziye2chen.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Neuronal Wiring Diagram of An Adult Brain</title><link href="https://ziye2chen.github.io/blog/2024/Neuro/" rel="alternate" type="text/html" title="Neuronal Wiring Diagram of An Adult Brain"/><published>2024-10-08T00:01:00+00:00</published><updated>2024-10-08T00:01:00+00:00</updated><id>https://ziye2chen.github.io/blog/2024/Neuro</id><content type="html" xml:base="https://ziye2chen.github.io/blog/2024/Neuro/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture1-480.webp 480w,/assets/img/Neuron/Picture1-800.webp 800w,/assets/img/Neuron/Picture1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="introduction">Introduction</h2> <p>This paper focuses on studying the brain of <strong>Drosopila Melanogaster</strong>, which is often refered to as the <strong>fruit fly</strong>. In the paper, the researchers present a neuronal wiring diagram of a whole brain containing \(5 \times 10^7\) chemical synapses between \(139,255\) neuron reconstructed from an adult female Drosopila melanogaster.</p> <div class="row mt-3 text-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture2-480.webp 480w,/assets/img/Neuron/Picture2-800.webp 800w,/assets/img/Neuron/Picture2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture2.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The researchers derive a <strong>projectome</strong>, which is a map of projections between region, from the connectome and report on tracing of synaptic pathways and the analysis of information flow from inputs to outputs across both hemispheres and between the central brain and optic lobes.</p> <p>This paper also illustrate how structure can uncover putative circuit mechanisms underlying seasorimotor behaviors.</p> <h2 id="data-source">Data Source</h2> <p>The images of the entire adult female fly brain were previously acquired from the research by Zheng et al.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture3-480.webp 480w,/assets/img/Neuron/Picture3-800.webp 800w,/assets/img/Neuron/Picture3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture3.png" class="img-fluid rounded z-depth-1 w-70" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The authors of this paper previously realigned the electron microscopy images, automatically segmented all neurons in the images, created a computational system that allow interactive proofreading of segmentation, and assembled an online community: <strong>Flywire</strong>.</p> <ul> <li>Link to Flywire: <a href="https://flywire.ai">https://flywire.ai</a></li> <li>Link to the paper of Flywire: <a href="https://www.nature.com/immersive/d42859-024-00053-4/index.html">https://www.nature.com/immersive/d42859-024-00053-4/index.html</a></li> </ul> <h2 id="motivation-and-background">Motivation and Background</h2> <p>Why we need to map the neuronal wiring of the brain?</p> <p>Neuroscientists have long recognized that understanding the global connectivity of neurons is the key to understanding how the brain works.</p> <p>Electron microscopic brain images are applied to chunks of brains to reconstruct local connectivity maps. But nevertheless inadequate for understanding brain function more globally.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture4-480.webp 480w,/assets/img/Neuron/Picture4-800.webp 800w,/assets/img/Neuron/Picture4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture4.png" class="img-fluid rounded z-depth-1 w-60" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And why choose fruit fly as research subject?</p> <p>Because although small, its brain contain \(10^5\) neurons and \(10^8\) synapses that enable a fly to see, smell, walk and fly. And flies engage in dynamic social interaction, navigate over distance and form long-term memories.</p> <h2 id="neurons">Neurons</h2> <p>Of the 139,255 proofread neurons in FlyWire, 118,501 are <strong>intrinsic</strong> to the brain, which is defined as the central brain and optic lobes. Intrinsic neurons of the brain make up three-quarters of the adult fly nervous system and amount to 85% of brain neurons.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture5-480.webp 480w,/assets/img/Neuron/Picture5-800.webp 800w,/assets/img/Neuron/Picture5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And brain communicates primarily with itself, and only secondarily with the outside world.</p> <p>Brain neurons that are not instrinsic can be divided into two categories, <strong>afferent</strong> and <strong>efferent</strong> depending on the locations of their cell bodies.</p> <h2 id="afferent-and-efferent-neurons">Afferent and Efferent Neurons</h2> <ul> <li> <p>Afferent (sensory and ascending) neurons: the cell body is outside the brain</p> </li> <li> <p>Efferent (descending, motor and endocrine) neurons: the cell body is contained in the brain</p> </li> </ul> <p>It is generally accurate to think of an afferent neuron as a brain input, and an efferent neuron as a brain output.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture6-480.webp 480w,/assets/img/Neuron/Picture6-800.webp 800w,/assets/img/Neuron/Picture6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="optic-lobes-and-central-brain">Optic Lobes and Central Brain</h2> <p><strong><em>Optic lobe refers to brain structures involved in vision</em></strong></p> <p>Visual afferents are by far the most numerous type of sensory input and enter the brain directly rather than through nerves. Of the 118,501 intrinsic neurons, 32,388 are fully contained in the central brain and 77,536 are fully contained in the optic lobes and ocellar ganglia. This number excludes the photorecepter, which are sensory afferent neurons.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture7-480.webp 480w,/assets/img/Neuron/Picture7-800.webp 800w,/assets/img/Neuron/Picture7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The domination of the count by visual areas reflects the nature of Drosophila as a highly visual animal. It shows that vision plays a significant role in neurons and brain.</p> <h2 id="synapses-and-connections">Synapses and Connections</h2> <p>A Drosophila synapse is generally polyadic, meaning that a single presynapse communicates with multiple target postsynapses. In the research, they define a connection from neuron A to neuron B as the set of synapses from A to B.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture8-480.webp 480w,/assets/img/Neuron/Picture8-800.webp 800w,/assets/img/Neuron/Picture8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture8.png" class="img-fluid rounded z-depth-1 w-60" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Setting a threshold of at least 5 synapses for determining a strong connection is likely to be adequate for avoiding false positive in the dataset while not missing connections.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture9-480.webp 480w,/assets/img/Neuron/Picture9-800.webp 800w,/assets/img/Neuron/Picture9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="projectome">Projectome</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture10-480.webp 480w,/assets/img/Neuron/Picture10-800.webp 800w,/assets/img/Neuron/Picture10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture10.png" class="img-fluid rounded z-depth-1 w-60" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The researchers computed a projectome, which is a neuropil-neuropil matrix, from a synapse-level connectome. Then they weighted neuron projections by the product of the respective number of synapses and normalized the result for every neuron. And they added afferent and efferent neurons to the matrix by calculating the sum of the weighted neuron projections per superclass to and from all neuropils, respectively.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture11-480.webp 480w,/assets/img/Neuron/Picture11-800.webp 800w,/assets/img/Neuron/Picture11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture11.png" class="img-fluid rounded z-depth-1 w-75" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture12-480.webp 480w,/assets/img/Neuron/Picture12-800.webp 800w,/assets/img/Neuron/Picture12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="analysis-of-information-flow">Analysis of Information Flow</h2> <p>Although afferent and efferent neurons make up a small proportion of the brain (13.9% and 1.1%), they connect the brain to the outside world.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture13-480.webp 480w,/assets/img/Neuron/Picture13-800.webp 800w,/assets/img/Neuron/Picture13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture13.png" class="img-fluid rounded z-depth-1 w-70" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Firstly, the researchers used a probabilistic model to estimate information flow in the connectome, starting from a set of seed neurons. The likelihood of a neuron being traversed increases with the fraction of inputs from already traversed neurons up to an input fraction of 30%, after which traversal is guaranteed.</p> <p>Next, they measured the flow distance from these starting neurons to all intrinsic and efferent neurons of the central brain.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture14-480.webp 480w,/assets/img/Neuron/Picture14-800.webp 800w,/assets/img/Neuron/Picture14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture14.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Finally, to visualize information flow for neurons with inputs in the central brain in a common space, they treated the traversal distances starting from each seed population as a neuron embedding and built a uniform manifold approximation and projection (UMAP) from all of these embeddings</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Neuron/Picture15-480.webp 480w,/assets/img/Neuron/Picture15-800.webp 800w,/assets/img/Neuron/Picture15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Neuron/Picture15.png" class="img-fluid rounded z-depth-1 w-70" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conlustion">Conlustion</h2> <p>Connectome analysis:</p> <ul> <li>The technologies and open ecosystem reported here set the stage for future large-scale connectome projects in other species</li> <li>Has significant benefits for brain research and enables many kinds of studies that were not previously possible using wiring diagrams of portions of the fly brain</li> </ul> <p>Limitation:</p> <ul> <li>The observed synapse counts underrepresent the actual number of synapses and some connections with few synapses remain undetected</li> </ul> <p>For AI</p> <ul> <li>The efficient information processing capabilities of the fruit fly brain could provide inspiration for the design of Neural Networks and Large Language Models</li> </ul>]]></content><author><name></name></author><category term="others"/><category term="paperReading"/><summary type="html"><![CDATA[My presentation on "neuronal wiring diagram of an adult brain"]]></summary></entry><entry><title type="html">Encoder</title><link href="https://ziye2chen.github.io/blog/2024/Encoder/" rel="alternate" type="text/html" title="Encoder"/><published>2024-06-20T00:01:00+00:00</published><updated>2024-06-20T00:01:00+00:00</updated><id>https://ziye2chen.github.io/blog/2024/Encoder</id><content type="html" xml:base="https://ziye2chen.github.io/blog/2024/Encoder/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/encoder/transformer-480.webp 480w,/assets/img/encoder/transformer-800.webp 800w,/assets/img/encoder/transformer-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/encoder/transformer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="token-embedding">Token Embedding</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TokenEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TokenEmbedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <h2 id="position-embedding">Position Embedding</h2> <p>Position embedding is essential, and its mathematical formula is well worth your time. With position embedding, we can input a sentence of different lengths and use the same embedding math formula to embed its position information.</p> <p>Let \(t\) be the desired position in an input sentence, \(\vec{p}_t \in \mathbb{R}^d\) be its corresponding encoding, and \(d\) be the encoding dimension (where \(d \geq 0\)). Then \(f : \mathbb{N} \rightarrow \mathbb{R}^d\) will be the function that produces the output vector \(\vec{p}_t\) and it is defined as follows:</p> \[\vec{p}_t^{(i)} = f(t)^{(i)} := \begin{cases} \sin(\omega_i \cdot t), &amp; \text{if } i = 2k \\ \cos(\omega_i \cdot t), &amp; \text{if } i = 2k + 1 \end{cases}\] <p>where</p> \[\omega_i = \frac{1}{10000^{2i/d}}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PositionalEmbedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">maxlen</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="c1"># initialize the encoding
</span>        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># This encoding does not require a gradient
</span>
        <span class="c1"># generate the position (The most important one in positional embedding!!!):
</span>        <span class="n">pos</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="c1"># generate a series from 0 to maxlen-1 [0, 1, ... , maxlen-1]
</span>        <span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span><span class="p">.</span><span class="nf">float</span><span class="p">().</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># add a dimension [[0.], [1.], ... , [maxlen-1]]
</span>        <span class="n">_2i</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="c1"># generate 2i: [0, 2, 4, ...]
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">(</span><span class="n">_2i</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">(</span><span class="n">_2i</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># there is no need to RETURN all the arguments, so make a cut with seq_len
</span>        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">[:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>
</code></pre></div></div> <h2 id="layer-norm">Layer Norm</h2> <p><strong><em>Why do we need normalization</em></strong></p> <p>In deep neural networks, the layers will directly or indirectly affect each other; a slight change in one layer may lead to different layers of violent shock, resulting in the corresponding network layer falling into saturation and model training difficulties. This phenomenon is called “Internal Covariate Shift.” [For example, in the sigma function, when \(x&gt;10\), the gradient value is close to 0, and the gradient of the lower neural network disappears in the BP process].</p> <p>In order to reduce this effect, so it is handled in terms of intuitive data distribution by normalizing the batch data to an \(N(0,1)\) distribution, which allows for a manageable range of input data distributions for each layer.</p> <p>Two normalization methods are often used in deep neural networks: batch normalization and layer normalization.</p> <p>Batch normalization is generally used in the CV domain, while layer normalization is generally used in the NLP domain.</p> <p>The equations of these two normalization methods are formally the same:</p> <p>\(N(x) = \gamma \left(\frac{x - \mu(x)}{\sigma(x)}\right) + \beta\) where \(\gamma\) and \(\beta\) are affine parameters learned from data, \(\mu (x)\) and \(\sigma (x)\) are the mean and standard deviation. Batch normalization computes \(\mu (x)\) and \(\sigma (x)\) across batch size and spatial dimensions independently for each feature channel. However, layer normalization computes \(\mu (x)\) and \(\sigma (x)\) across all channels for each sample.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/encoder/layerNorm-480.webp 480w,/assets/img/encoder/layerNorm-800.webp 800w,/assets/img/encoder/layerNorm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/encoder/layerNorm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As in NLP the above figure of C, N, H,W:</p> <ul> <li>N: N sentences, i.e. batchsize;</li> <li>C: length of a sentence, i.e. seqlen;</li> <li>H,W: word vector dimension embedding dim.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">out</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="c1"># Finally, the result is scaled and offset.
</span>        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <h2 id="ffn">FFN</h2> <p>FFN is essentially a two-layer MLP. The mathematical equation of this MLP is:</p> \[FFN(x) = \max (0, x \cdot W_1 + b_1) \cdot W_2 + b_2\] <p>FFN can increase the expressive power of a model by adding a nonlinear transformation between two FCs, allowing the model to capture complex features and patterns.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PositionwiseFeedForward</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="transformer-embedding">Transformer Embedding</h2> <p>The Token Embedding module and Positional Embedding module are put together to form the Transformer Embedding module.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">drop_prob</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerEmbedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tok_emb</span> <span class="o">=</span> <span class="nc">TokenEmbedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pos_emb</span> <span class="o">=</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">drop_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">drop_prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tok_emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pos_emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">drop_out</span><span class="p">(</span><span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span><span class="p">)</span>
</code></pre></div></div> <h2 id="multi-head-attention">Multi-Head Attention</h2> <p>The detail about Multi-Head Attention can be found in <a href="https://ziye2chen.github.io/blog/2024/attention/">here</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_head</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w_combine</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">dimension</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">n_d</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_q</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_k</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_v</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">n_d</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">n_d</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">n_d</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="n">score</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">n_d</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># mask = torch.tril(torch.ones(time, time, dtype=bool))
</span>            <span class="n">score</span> <span class="o">=</span> <span class="n">score</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">score</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>

        <span class="n">score</span> <span class="o">=</span> <span class="n">score</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">dimension</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_combine</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outpu</span>
</code></pre></div></div> <h2 id="encoder-layer">Encoder Layer</h2> <p>Putting all the parts together makes up the Encoder part of the Transformer.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/encoder/encoder-480.webp 480w,/assets/img/encoder/encoder-800.webp 800w,/assets/img/encoder/encoder-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/encoder/encoder.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">ffn_hidden</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">drop_prob</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_head</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">drop1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">drop_prob</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">ffn_hidden</span><span class="p">,</span> <span class="n">drop_prob</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">drop2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">drop_prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">_x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">drop1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">_x</span><span class="p">)</span>

        <span class="n">_x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">drop2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">_x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="structure"/><category term="paperReading"/><summary type="html"><![CDATA[this is a blog about the encoder in transformer]]></summary></entry><entry><title type="html">Introduction to KAN</title><link href="https://ziye2chen.github.io/blog/2024/kan/" rel="alternate" type="text/html" title="Introduction to KAN"/><published>2024-06-01T00:01:00+00:00</published><updated>2024-06-01T00:01:00+00:00</updated><id>https://ziye2chen.github.io/blog/2024/kan</id><content type="html" xml:base="https://ziye2chen.github.io/blog/2024/kan/"><![CDATA[<h2 id="kan-and-mlp">KAN and MLP</h2> <p>The emergence of the <a href="https://arxiv.org/pdf/2404.19756">KANs (Kolmogorov-Arnold Networks)</a> model has led to heated discussions in the AI community, and the reason for this is that the flaws in the structure of the MLPs itself have been troubling the researchers involved.</p> <p>Nowadays, most AI research is based on MLP. The appearance of KANs can potentially revolutionize the direction of AI research. If KANs are as marvelous as expected in subsequent research, their appearance will be like the emergence of cement in a society that has been only using wood to build houses.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/2_1-480.webp 480w,/assets/img/KAN/2_1-800.webp 800w,/assets/img/KAN/2_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/2_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="the-drawbacks-of-mlp">The drawbacks of MLP</h2> <p>The theoretical basis for MLPs is that a single hidden layer network containing enough neurons can approximate any continuous function. But MLPs have some natural flaws:</p> <h3 id="vanishing-and-exploding-gradients">Vanishing and exploding gradients</h3> <p><strong><em>Vanishing gradients:</em></strong> the gradient tends to zero, the network weights cannot be updated or are updated very slightly, and the network will not be effective even if it is trained for a long time;</p> <p><strong><em>Exploding gradient:</em></strong> the gradient grows exponentially and becomes very large, leading to a large update of the network weights, making the network unstable.</p> <p>Whether the gradient disappears or the gradient explosion is essentially due to the backpropagation of the deep neural network.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/NN-480.webp 480w,/assets/img/KAN/NN-800.webp 800w,/assets/img/KAN/NN-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/NN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For example, in a four-layer neural network, its backpropagation formula is as follows:</p> \[\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial y_4} \frac{\partial y_4}{\partial z_4} \frac{\partial z_4}{\partial y_3} \frac{\partial y_3}{\partial z_3} \frac{\partial z_3}{\partial y_2} \frac{\partial y_2}{\partial z_2} \frac{\partial z_2}{\partial y_1} \frac{\partial y_1}{\partial z_1} \frac{\partial z_1}{\partial w_1}\] \[\frac{\partial L}{\partial y_4} = \sigma'(z_4) w_4 \sigma'(z_3) w_3 \sigma'(z_2) w_2 \sigma'(z_1) x_1\] <p>where \(y_i = \sigma(z_i) = \sigma(w_i x_i + b_i)\), which \(x_i = y_{i-1}\), and \(\sigma\) is <strong><em>sigmoid</em></strong> activation function.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/sigmoid-480.webp 480w,/assets/img/KAN/sigmoid-800.webp 800w,/assets/img/KAN/sigmoid-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/sigmoid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>From the above, it can be seen that there is a sequence of multiplications in the backpropagation of the neural network. Also, the derivative of the sigmoid activation function has a value range of 0 to 0.25. So when each \(w_i\) is small, and there are many \(\sigma'\) multiplied together, the gradient tends to 0. When each \(w_i\) very large, the gradient becomes very large.</p> <p>While many methods say they solve this problem, the issue is still complicated to avoid completely, as it is an intrinsic shortcoming of MLPs.</p> <h3 id="inefficient-utilization-of-parameters">Inefficient utilization of parameters</h3> <p>MLPs usually use fully connected networks, so the number of parameters is huge. When the network becomes very deep, only a few parameters are utilized. And this is one reason why LLM may have reached a dead end.</p> <p>Although it is possible to simplify the structure using CNNs or regularization or the like, essentially, the structure of a highly dense continuous linear model coupled with an activation function dictates that the MLP can’t be too sparse or will not have enough representational power.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/cnn-480.webp 480w,/assets/img/KAN/cnn-800.webp 800w,/assets/img/KAN/cnn-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/cnn.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="inadequate-capacity-to-handle-high-dimensional-data-and-long-term-dependency-issues">Inadequate capacity to handle high-dimensional data and Long-term dependency issues</h3> <p>MLPs do not utilize the intrinsic structure of the data, such as local spatial correlations in images and sequence information in textual data. Although CNN and RNN can improve these problems, this base module of MLPs determines that these problems cannot be solved completely.</p> <p>MLPs also have difficulty capturing long-standing relationships in input sequences. (RNN &amp; Transformer can improve it)</p> <h2 id="kolmogorov-arnold-networks">Kolmogorov-Arnold Networks</h2> <h3 id="the-kolmogorov-arnold-representation-theorem">The Kolmogorov-Arnold representation theorem</h3> <p>KANs are inspired by the Kolmogorov-Arnold representation theorem. Vladimir Arnold and Andrey Kolmogorov established that if f is a multivariate continuous function on a bounded domain, then f can be written as a finite composition of continuous functions of a single variable and the binary operation of addition.</p> <p>More specifically, for a smooth \(f : [0, 1]^n \rightarrow \mathbb{R}\),</p> \[f(\mathbf{x}) = f(x_1, \dots, x_n) = \sum_{q=1}^{2n+1} \Phi_q \left( \sum_{p=1}^n \phi_{q,p}(x_p) \right)\] <p>where \(\phi_{q,p} : [0,1] \rightarrow \mathbb{R}\) and \(\Phi_q : \mathbb{R} \rightarrow \mathbb{R}\). This theorem reveals how any multivariate continuous function can be represented by a simpler set of functions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/2_2-480.webp 480w,/assets/img/KAN/2_2-800.webp 800w,/assets/img/KAN/2_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/2_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It can be shown that the Kolmogorov-Arnold representation theorem is a simple KANs which has only two-layer nonlinearities and a small number of terms (2n + 1) in the hidden layer. This structure appeared a long time ago, but the difficulty is how to make the KANs deeper. And two-layer networks are too simple to approximate complex functions. (Details of deepening KANs will be mentioned later)</p> <h3 id="why-kans-are-awesome">Why KANs are awesome</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/0_1-480.webp 480w,/assets/img/KAN/0_1-800.webp 800w,/assets/img/KAN/0_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/0_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>KANs contrast MLPs by not having a nested relationship between activation functions and parameter matrices but rather a nesting of directly nonlinear functions \(\Phi\). And \(\Phi\) all use the same function structure. (In the paper, they use the spline function in numerical analysis)</p> \[KAN(x) = (\Phi_{L-1} \circ \Phi_{L-2} \circ \cdots \circ \Phi_1 \circ \Phi_0)x\] \[MLP(x) = (W_{L-1} \circ \sigma \circ W_{L-2} \circ \sigma \circ \cdots \circ W_1 \circ \sigma \circ W_0)x\] <p>When learning the network parameters, MLPs learn the linear function with a fixed nonlinear activation function. In contrast, KANs learn the parameterized nonlinear function directly, which enhances their characterization ability.</p> <p>Because of the parameters’ complexity, although individual spline functions are more complex to learn than linear functions, KANs typically allow for smaller computational graphs than MLPs (i.e., a smaller network size is required to achieve the same effect).</p> <h3 id="the-details-about-kans">The details about KANs</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/spline-480.webp 480w,/assets/img/KAN/spline-800.webp 800w,/assets/img/KAN/spline-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/spline.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If MLPs fit a function with some lines, KANs fit a function with some curves. In KANs, the spline function is used as the nonlinear function \(\Phi\).</p> <p>The KAN layer is simply a matrix of functions \(\Phi = \{ \phi_{q,p} \}\), \(p=1,2,\cdots,n_{\text{in}}\), $$q=1,2,\cdots,n_{\text{out}}.</p> \[\Phi_l = \left( \begin{array}{ccc} \phi_{l,1,1}(\cdot) &amp; \phi_{l,1,2}(\cdot) &amp; \cdots &amp; \phi_{l,1,n_l}(\cdot) \\ \phi_{l,2,1}(\cdot) &amp; \phi_{l,2,2}(\cdot) &amp; \cdots &amp; \phi_{l,2,n_l}(\cdot) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \phi_{l,n_{l+1},1}(\cdot) &amp; \phi_{l,n_{l+1},2}(\cdot) &amp; \cdots &amp; \phi_{l,n_{l+1},n_l}(\cdot) \end{array} \right)\] \[X_{l+1} = \Phi_l X_l\] <p>where \(\Phi_l\) is the function matrix corresponding to the \(l_{\text{th}}\) KAN layer.</p> <p>Although the KAN layer looks very simple, it is not easy to make it well optimizable. The key tricks are as follows:</p> <h4 id="1-residual-activation-functions">1. Residual activation functions：</h4> \[\phi(x) = w \big(b(x) + \text{spline}(x)\big)\] <p>\(b(x)\) is a basis function,</p> \[b(x) = \text{SiLU}(x) = \frac{x}{1 + e^{-x}}\] <p>SiLU (Sigmoid Linear Unit) function is an improved version of Sigmoid and ReLU.SiLU has the properties of having no upper and lower bounds, and it is smooth and non-monotonic.SiLU works better than ReLU on deep models. It can be seen as a smooth ReLU activation function.SiLU activation function is also known as the Swish activation function, an adaptive activation function introduced by Google Brain in 2017.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/SiLU-480.webp 480w,/assets/img/KAN/SiLU-800.webp 800w,/assets/img/KAN/SiLU-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/SiLU.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>\(\text{spline}(x)\) is parametrized as a linear combination of B-splines such that</p> \[\text{spline}(x) = \sum_i c_i B_i(x)\] <p>where every \(c_i\) is trainable. The B-spline function \(\text{spline}(x)\) can be considered a linear combination of control points \(c_i\) and a weighting function \(B_i(x)\). In short, the B-spline curve is the weighted sum of each control point multiplied by its corresponding weight function. The weighting function is predefined and is only related to order D, and it does not change as the control points change.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/bSpline-480.webp 480w,/assets/img/KAN/bSpline-800.webp 800w,/assets/img/KAN/bSpline-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/bSpline.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In principle \(w\) is redundant since it can be absorbed into \(b(x)\) and \(\text{spline}(x)\). However \(w\) can better control the overall magnitude of the activation function.</p> <h4 id="2-initialization-scales">2. Initialization scales:</h4> <p>Each activation function is initialized to have \(\text{spline}(x) \approx 0\).</p> <p>\(w\) is initialized according to Xavier initialization. Xavier initialization is a very effective method for initializing neural networks, and the method originates from a 2010 paper, <a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a>.</p> <p>In the paper, Xavier Glorot offers the insight that the variance of the activation values decreases layer by layer, which results in the gradient in backpropagation and also decreases layer by layer. The solution to the vanishing gradient is to avoid the decay of the variance of the activation values and, ideally, to keep the output values (activation values) of each layer Gaussian distributed. So they initialized the weights \(W\) at each layer with the following commonly used heuristic:</p> \[W \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_j + n_{j+1}}}, \frac{\sqrt{6}}{\sqrt{n_j + n_{j+1}}}\right]\] <p>where \(n_j\) is the number of inputs, and \(n_{j+1}\) is the number of outputs.</p> <p>In addition, each KAN layer has an equal layer width, with L layers and N nodes per layer.</p> <p>Each spline function is typically of order k = 3, G intervals, and G + 1 grid points.</p> <p>Thus KANs require a total of \(O(N^2L(G+k))\) or \(O(N^2LG)\) parameters, while MLPs require \(O(N^2L)\) parameters. It may seem that the computational complexity of KANs is greater than that of MLPs, but KANs require much less \(N\) than MLPs, which not only saves parameters but also improves generalization and aids interpretability.</p> <h4 id="3-update-of-spline-grids">3. Update of spline grids:</h4> <p>In KANs, they update each grid on the fly according to its input activations, to address the issue that splines are defined on bounded regions but activation values can evolve out of the fixed region during training.</p>]]></content><author><name></name></author><category term="AI"/><category term="structure"/><category term="paperReading"/><summary type="html"><![CDATA[this is an introduction about KAN]]></summary></entry><entry><title type="html">The detail about Attention structure</title><link href="https://ziye2chen.github.io/blog/2024/attention/" rel="alternate" type="text/html" title="The detail about Attention structure"/><published>2024-05-23T00:01:00+00:00</published><updated>2024-05-23T00:01:00+00:00</updated><id>https://ziye2chen.github.io/blog/2024/attention</id><content type="html" xml:base="https://ziye2chen.github.io/blog/2024/attention/"><![CDATA[<h2 id="attention-structure">Attention Structure</h2> <h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3> <p>The attention architecture derives from the mechanisms of human attention. When an image is placed in front of a human, the human scans the global image to obtain areas that are worth focusing on, and devotes more attention to these areas to obtain information. Nowadays, the popular attention model generally relies on the encoder-decoder framework, which can deal with tasks including NLP, image processing, etc.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/scaledAndMultiAttention-480.webp 480w,/assets/img/attention/scaledAndMultiAttention-800.webp 800w,/assets/img/attention/scaledAndMultiAttention-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/scaledAndMultiAttention.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Scaled Dot-Product Attention is the most basic attention structure. Multi-Head Attention is multiple parallel Scaled Dot-Product Attention, which splices the output of each Scaled Dot-Product Attention and does a linear transformation to output.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/transform-480.webp 480w,/assets/img/attention/transform-800.webp 800w,/assets/img/attention/transform-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/transform.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Attention can be considered as a specific word weighting, given a sequence of inputs \(\{x_1, x_2, \dots, x_n\}\), after the attentions layer, combining the input individuals \(\{x_1, x_2, \dots, x_n\}\) and outputs \(\{y_1, y_2, \dots, y_n\}\) will be as follows:</p> <ol> <li>The input features are multiplied by three sets of weight matrices \(W^Q, W^K, W^V\) to generate the three matrices of query, key, and value;</li> <li>The attention weight matrix \(A\) is obtained from the product of a certain \(Q\) and \(K\), which is normalized to get \(\hat{A}\);</li> <li>The normalized weights \(\hat{A}\) are multiplied by V to get the final output feature \(O\).</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/qkv-480.webp 480w,/assets/img/attention/qkv-800.webp 800w,/assets/img/attention/qkv-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/qkv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Mathematically, the self-attention matrix for input matrices \((Q, K, V)\) is calculated as:</p> \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/Y-480.webp 480w,/assets/img/attention/Y-800.webp 800w,/assets/img/attention/Y-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/Y.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="multi-head-attention">Multi-Head Attention</h3> <p>If each vector \(q_i, k_i, v_i\) is split into n, we can get the n-headed attention mechanism. It is recognized that multi-head self-attention mechanisms are better than single-head ones because it can capture information in more dimensions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/multihead-480.webp 480w,/assets/img/attention/multihead-800.webp 800w,/assets/img/attention/multihead-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/multihead.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>At the beginning, we need to import some libraries we will normally use.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">math</span>
</code></pre></div></div> <p>We need some data for testing. The dimension of testing data is 3.</p> <ul> <li>The first dimension represents batch.</li> <li>The second dimension represents time.</li> <li>The third dimension represnts the dimension after Encoder.</li> </ul> <p>And in language model, the ‘512’ below normally means the dimensions of the word vector do you want to map your word to after Embedding.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">512</span><span class="p">)</span> <span class="c1">#Batch, Time, Dimension
</span><span class="nf">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <p>Next, we set up the basic parameters of Multihead attention.</p> <ul> <li>d_model represents the number of dimensions I want to map to the QKV space.</li> <li>n_head repersents the number of head.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">n_head</span> <span class="o">=</span> <span class="mi">8</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">multi_head_attention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
<span class="c1"># When we are writing a pytorch class, we need to inherit nn.Module
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_head</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">multi_head_attention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span> <span class="c1"># Initialize some basic parameters
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>

        <span class="n">self</span><span class="p">.</span><span class="n">w_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span> <span class="c1">#Query
</span>        <span class="n">self</span><span class="p">.</span><span class="n">w_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span> <span class="c1">#Key
</span>        <span class="n">self</span><span class="p">.</span><span class="n">w_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span> <span class="c1">#Value
</span>        <span class="c1"># It's like we are looking for some queries to match some keys, and values are then weighted to combine.
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w_combine</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="c1"># Because of 'multi-head', we need a combinatorial mapping of the outputs.
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Still in class multi_head_attention(nn.Module):
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">dimension</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span> <span class="c1"># get the dimensions of Q, K, V
</span>        <span class="n">n_d</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span> <span class="c1"># Because we have n heads, 'd_model' for each submodel has to be divisible by 'n_head'.
</span>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_q</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_k</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_v</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">n_d</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>   
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">n_d</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>   
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">n_d</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>   
        <span class="c1"># We cannot put 'n_head' in the last dimension because we need to process the last two dimensions.
</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">n_d</span><span class="p">)</span> <span class="c1"># The most important code in Attention.
</span>
        <span class="n">minusInfty</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10000</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">score</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">minusInfty</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">score</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>

        <span class="n">score</span> <span class="o">=</span> <span class="n">score</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">dimension</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_combine</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span> 
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">attention</span> <span class="o">=</span> <span class="nf">multi_head_attention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_head</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">attention</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <h2 id="something-you-might-want-to-know-about-attention">Something you might want to know about Attention</h2> <h3 id="the-square-root-of-d-in-the-attention-calculation">The square root of d in the Attention calculation</h3> <p><strong>Why should we divide \(\sqrt{d}\) in the Attention calculation?</strong></p> <p>Assume that the Query vector \(q\) and the Key vector \(k\) have dimension \(d\) and their elements are independent and identically distributed random variables with mean \(0\) and variance \(1\).</p> <p>Then their dot product \(q \cdot k\) can be expressed as: \(q \cdot k = \sum_{i=1}^d q_i k_i\). Since each \(q_i\) and \(k_i\) is an independently and identically distributed random variable, their product \(q_i k_i\) is also a random variable. For each \(q_i k_i\), the expectation is \(0\) and the variance is \(1\).</p> <p>Therefore, the expected value of the dot product \(q \cdot k\) is:</p> \[\mathbb{E}[q \cdot k] = \mathbb{E} \left[ \sum_{i=1}^d q_i k_i \right] = \sum_{i=1}^d \mathbb{E}[q_i k_i] = 0\] <p>Moreover, its variance is:</p> \[\text{Var}(q \cdot k) = \text{Var} \left( \sum_{i=1}^d q_i k_i \right) = \sum_{i=1}^d \text{Var}(q_i k_i) = d\] <p>Thus, the standard deviation of the dot product \(q \cdot k\) is:</p> \[\sigma = \sqrt{\text{Var}(q \cdot k)} = \sqrt{d}\] <p>By the Central Limit Theorem, the sum of these independently and identically distributed random variables will converge to a normal distribution when $d$ is large enough. To normalize this, we subtract the expected value and divide by the standard deviation:</p> \[\frac{q \cdot k - \mathbb{E}[q \cdot k]}{\sqrt{\text{Var}(q \cdot k)}}\] <p>Here, \(\mathbb{E}[q \cdot k] = 0\) and \(\text{Var}(q \cdot k) = d\), so the expression becomes:</p> \[\frac{q \cdot k}{\sqrt{d}}\] <p>This will make the expression approximately obey the standard normal distribution \(N(0, 1)\).</p>]]></content><author><name></name></author><category term="AI"/><category term="structure"/><category term="code"/><summary type="html"><![CDATA[this is the detail about attention structure]]></summary></entry><entry><title type="html">Modern Solution of Optimal Transport - Sinkhorn</title><link href="https://ziye2chen.github.io/blog/2024/sinkhorn/" rel="alternate" type="text/html" title="Modern Solution of Optimal Transport - Sinkhorn"/><published>2024-03-30T00:01:00+00:00</published><updated>2024-03-30T00:01:00+00:00</updated><id>https://ziye2chen.github.io/blog/2024/sinkhorn</id><content type="html" xml:base="https://ziye2chen.github.io/blog/2024/sinkhorn/"><![CDATA[<h2 id="modern-solution-of-optimal-transport---sinkhorn">Modern Solution of Optimal Transport - Sinkhorn</h2> <p>In 2013, Cuturi proposed a scalable approximation of optimal transport. Since then, OT is increasingly becoming a core tool of machine learning research toolbox.</p> <p>The computation of other optimal transport formulation involves the resolution of a linear program whose cost can quickly become prohibitive.</p> <p>But Sinkhorn distance looks at transport problems from a maximum-entropy perspective. It smooths the classic optimal transport problem with an entropic regularization term. And the computation through Sinkhorn’s matrix scaling algorithm is several orders of magnitude faster than that of transport solvers, which is \(O(n^2)\).</p> <h3 id="sinkhorn-distance">Sinkhorn Distance</h3> <p>In what follows, \(\left \langle \cdot , \cdot \right \rangle\) stands for the Frobenius dot-product. For two probability vectors \(\mu\) and \(\nu\) in the simplex \(\sum_d := \{ x \in \mathbb{R}_+^{d} : x^T \mathbf{1}_d=1 \}\), where \(\mathbf{1}_d\) is the d dimensional vector of ones, we write \(U(r,c)\) for the transport polytope of \(\mu\) and \(\nu\), namely the polyhedral set of \(d \times d\) matrices,</p> \[U(\mu,\nu) := \{ P \in \mathbb{R}^{d\times d}_+ | P \mathbf{1}_d = \mu, P^T \mathbf{1}_d = \nu \}\] <p>And we define the entropy $h$ and Kullback-Leibler divergences of \(P,Q\in U(\mu,\nu)\) and a marginals \(\mu \in \sum_d\) as</p> \[h(\mu) = -\sum_{i=1}^{d}\mu_i log \mu_i,\quad h(P) = -\sum_{i,j=1}^{d}p_{ij} log p_{ij}, \quad \mathbf{KL}(P||Q)=\sum_{ij}p_{ij} log\frac{p_{ij}}{q_{ij}}\] <p>where \(p(X=i,Y=j)=p_{ij}\) and the same as \(q_ij\).</p> <p>The following information theoretic inequality (Cover and Thomas, 1991, §2) for joint probabilities</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/3_1-480.webp 480w,/assets/img/optimalTransport/equation/3_1-800.webp 800w,/assets/img/optimalTransport/equation/3_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/3_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>is tight, since the independence table \(\mu\nu^T\) has entropy \(h(\mu\nu^T) = h(\mu)+h(\nu)\). By the concavity of entropy, we can introduce the convex set</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/3_2_3_3-480.webp 480w,/assets/img/optimalTransport/equation/3_2_3_3-800.webp 800w,/assets/img/optimalTransport/equation/3_2_3_3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/3_2_3_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <table> <tbody> <tr> <td>These two definitions are indeed equivalent, since one can easily check that $$\mathbf{KL}(P</td> <td> </td> <td>\mu\nu^T)= h(\mu)+h(\nu)-h(P)$$, a quantity which is also the mutual information $I(X</td> <td> </td> <td>Y)$ of two random variables \((X,Y)\) should they follow the joint probability \(P\) (Cover and Thomas, 1991, §2).</td> </tr> </tbody> </table> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/3_4-480.webp 480w,/assets/img/optimalTransport/equation/3_4-800.webp 800w,/assets/img/optimalTransport/equation/3_4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/3_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Why consider an entropic constraint in optimal transport? The first reason is computational. The second reason is built upon the following intuition. As a classic result of linear optimization, the OT problem is always solved on a vertex of \(U(\mu,\nu)\). Such a vertex is a sparse \(d\times d\) matrix with only up to \(2d-1\) non-zero elements (Brualdi, 2006, §8.1.3).</p> <p>When α is large enough, the Sinkhorn distance coincides with the classic OT distance. When α = 0, the Sinkhorn distance has a closed form and becomes a negative definite kernel if one assumes that M is itself a negative definite distance, or equivalently a Euclidean distance matrix.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/3_5-480.webp 480w,/assets/img/optimalTransport/equation/3_5-800.webp 800w,/assets/img/optimalTransport/equation/3_5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/3_5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/3_6-480.webp 480w,/assets/img/optimalTransport/equation/3_6-800.webp 800w,/assets/img/optimalTransport/equation/3_6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/3_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/3_7-480.webp 480w,/assets/img/optimalTransport/equation/3_7-800.webp 800w,/assets/img/optimalTransport/equation/3_7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/3_7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And \(d_{M,\alpha}(x,y)\) also satisfies the triangle inequality:</p> \[d_{M,\alpha}(x,z) \le d_{M,\alpha}(x,y)+d_{M,\alpha}(y,z)\] <h3 id="computing-regularized-transport-with-sinkhorns-algorithm">Computing Regularized Transport with Sinkhorn’s Algorithm</h3> <p>We consider in this section a Lagrange multiplier for the entropy constraint of Sinkhorn distances:</p> \[For \lambda &gt; 0, d_{M}^{\lambda}(\mu, \nu) := \langle P^{\lambda}, M \rangle, \text{where} \ P^{\lambda} = \underset{P \in U(\mu,\nu)}{\text{argmin}} \langle P, M \rangle - \frac{1}{\lambda}h(P).\] <p>By duality theory we have that to each \(\alpha\) corresponds a \(\lambda \in [0,\infty]\) such that \(d_{M,\alpha}(\mu, \nu)=d_{M}^{\lambda}(\mu, \nu)\) holds for that pair \((\mu,\nu)\). We call \(d_{M}^{\lambda}\) the dual-Sinkhorn divergence and show that it can be computed for a much cheaper cost than the original distance \(d_M\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/sinkhorn-480.webp 480w,/assets/img/optimalTransport/sinkhorn-800.webp 800w,/assets/img/optimalTransport/sinkhorn-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/sinkhorn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="pytorch-implementation-of-sinkhorn">PyTorch implementation of sinkhorn</h3> <p><a href="https://github.com/vlkit/vlkit/blob/main/vlkit/optimal_transport/sinkhorn.py">vlkit.optimal_transport.sinkhorn</a> has a PyTorch implementation of sinkhorn that allows us to compute and visualize the optimal transport between two distributions. As an example, two 1d Gaussian distributions are generated as source and target distributions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">matplotlib</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">gridspec</span>
<span class="kn">from</span> <span class="n">vlkit.optimal_transport</span> <span class="kn">import</span> <span class="n">sinkhorn</span>

<span class="c1"># generate two gaussians as the source and target
</span><span class="k">def</span> <span class="nf">gaussian</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">std</span><span class="o">**</span><span class="mi">2</span><span class="p">)).</span><span class="nf">exp</span><span class="p">()</span>
    <span class="n">d</span> <span class="o">/=</span> <span class="n">d</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">d</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">d1</span> <span class="o">=</span> <span class="nf">gaussian</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">d2</span> <span class="o">=</span> <span class="nf">gaussian</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="n">dist</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">abs</span><span class="p">().</span><span class="nf">float</span><span class="p">()</span>
<span class="n">dist</span> <span class="o">/=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span>

<span class="c1"># visualize distr
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">bar</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">d1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Source distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">bar</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">d2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Target distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/distribution-480.webp 480w,/assets/img/optimalTransport/distribution-800.webp 800w,/assets/img/optimalTransport/distribution-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">T</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="nf">sinkhorn</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="n">d1</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="n">d2</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">reg</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="p">.</span><span class="nc">GridSpec</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">d2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Target distribution</span><span class="sh">'</span><span class="p">)</span>

<span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">barh</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">d1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Source distribution</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">invert_xaxis</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">invert_yaxis</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="n">sharex</span><span class="o">=</span><span class="n">ax1</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="n">ax2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">T</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/transport-480.webp 480w,/assets/img/optimalTransport/transport-800.webp 800w,/assets/img/optimalTransport/transport-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/transport.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="optimal-transport"/><category term="math"/><category term="code"/><category term="optimalTransport"/><summary type="html"><![CDATA[this is my study note about sinkhorn]]></summary></entry><entry><title type="html">Optimal Transport</title><link href="https://ziye2chen.github.io/blog/2024/optimal-transport/" rel="alternate" type="text/html" title="Optimal Transport"/><published>2024-03-23T00:01:00+00:00</published><updated>2024-03-23T00:01:00+00:00</updated><id>https://ziye2chen.github.io/blog/2024/optimal-transport</id><content type="html" xml:base="https://ziye2chen.github.io/blog/2024/optimal-transport/"><![CDATA[<h2 id="intro-to-optimal-transport">Intro to Optimal Transport</h2> <p>Optimal transport problem can be understood as a problem of two piles of soil, shoveled from soil A to the other and eventually piled up to form soil B. And your mission is to find the most efficient method to shovel the soil.</p> <p>There are two ways to formulate the optimal transport problem: the Monge and Kantorovich formulations. Historically the Monge formulation comes before Kantorovich. The Kantorovich formulation can be seen as a generalisation of Monge. Both formulations have their advantages and disadvantages. Monge is more useful in applications, whilst Kantorovich is more useful theoretically.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/soil-480.webp 480w,/assets/img/optimalTransport/soil-800.webp 800w,/assets/img/optimalTransport/soil-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/soil.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="measure">Measure</h3> <p>In mathematics, measure is a generalization and formalization of geometrical measures like length, area, volume, magnitude, mass and probability. In Lebesgue measure, for lower dimensions n = 1, 2, or 3, it coincides with the standard measure of length, area, or volume.</p> <p>The concept of measures will be used later, and before I introduce probability measures, I need to introduce the histogram. The histogram here is the probability of n sums of 1, representing a probability distribution. We will use interchangeably the terms histogram and probability vector for any element $a \in \Sigma_n $ that belongs to the probability simplex</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_1-480.webp 480w,/assets/img/optimalTransport/equation/1_1-800.webp 800w,/assets/img/optimalTransport/equation/1_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And we use discrete measure describes a probability measure if, additionally, \(a \in \Sigma_n\) and more generally a positive measure if all the elements of vector \(a\) are non-negative.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_2-480.webp 480w,/assets/img/optimalTransport/equation/1_2-800.webp 800w,/assets/img/optimalTransport/equation/1_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And to avoid degeneracy issues where locations with no mass are accounted for, we will assume when considering discrete measures that all the elements of \(a\) are positive.</p> <p>A convenient feature of OT is that it can deal with measures that are either or both discrete and continuous within the same framework. So we can find a relation between discrete measures and general measures.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_3-480.webp 480w,/assets/img/optimalTransport/equation/1_3-800.webp 800w,/assets/img/optimalTransport/equation/1_3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="the-monge-formulation">The Monge Formulation</h3> <p>In optimal transport, there are two measures \(\mu\) and \(\nu\). You can imagine they are two piles of soil (but I will call them measure for the rest). And you can also consider the first measure \(\mu\) as a pile of sand and the second measure \(\nu\) as a hole we wish to fill up. We assume that both measures are probability measures on spaces $X$ and $Y$ respectively. The cost function \(c(x,y)\), \(c:X \to [0,+\infty]\), measures the cost of transporting one unit of mass from \(x \in X\) to \(y \in Y\). Now, the problem is to find the most optimal method.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_4-480.webp 480w,/assets/img/optimalTransport/equation/1_4-800.webp 800w,/assets/img/optimalTransport/equation/1_4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For greter generality we work with the inverse of \(T\) itself. The inverse is treated in the general set valued sense, i.e. \(x \in T^{-1}(y)\) if \(T(x)=y\), if the function \(T\) is injective then we can equivalently say that \(\nu (T(A)) = \mu (A)\) for all \(\mu\)-measurable \(A\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/T-480.webp 480w,/assets/img/optimalTransport/T-800.webp 800w,/assets/img/optimalTransport/T-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/T.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_5_1_6-480.webp 480w,/assets/img/optimalTransport/equation/1_5_1_6-800.webp 800w,/assets/img/optimalTransport/equation/1_5_1_6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_5_1_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Transport map \(T\) does not always exist. For example, there are two discrete measures \(\mu = \delta_{x_1}\), \(\nu = \frac{1}{2} \delta_{y_1} + \frac{1}{2} \delta_{y_2}\), where \(y_1 \not= y_2\). Then \(\nu({y_1}) = \frac{1}{2}\) but \(\mu(T^{-1}(y_1)) \in \{0,1\}\) depending on whether \(x \in T^{-1}(y_1)\). Hence no transport maps exist.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_7-480.webp 480w,/assets/img/optimalTransport/equation/1_7-800.webp 800w,/assets/img/optimalTransport/equation/1_7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The original monge’s optimal transport problem used \(L^1\) cost, i.e. \(c(x,y) = \mid x - y \mid\). But it is much harder to solve than with \(L^2\) cost, i.e. \(c(x,y) = \mid x - y \mid^2\).</p> <p>If the measure is discrete measure, we can also define the Monge’s optimal transport problem between discrete measures.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_8-480.webp 480w,/assets/img/optimalTransport/equation/1_8-800.webp 800w,/assets/img/optimalTransport/equation/1_8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="the-kantorovich-formulation">The Kantorovich Formulation</h3> <p>In the example \(\mu = \delta_{x_1}\), \(\nu = \frac{1}{2} \delta_{y_1} + \frac{1}{2} \delta_{y_2}\), it causes problem in Monge formulation because mass cannot be split. To allow mass to be split, the Kantorovich formulation appears, which has a natural relaxation. To formalize this, we consider a measure \(\pi \in \mathcal{P}(X \times Y)\) and think of \(d\pi(x,y)\) as the amount of mass transferred from $x$ to $y$. And we have the constrains:</p> <p>\(\pi(A \times Y) = \mu (A)\), \(\pi(X \times B) = \nu (B)\) for all mesaurable sets \(A \subseteq X\), \(B \subseteq Y\).</p> <p>And we denote the set of such \(\pi\) by \(\Pi(\mu,\nu)\) and call \(\Pi(\mu,\nu)\) the set of transport plans between \(\mu\) and \(\nu\). \(\Pi(\mu,\nu)\) is never non-empty and \(\mu \otimes \nu \in \Pi(\mu,\nu)\). Now we can define Kantorovich’s formulation of optimal transport.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_9-480.webp 480w,/assets/img/optimalTransport/equation/1_9-800.webp 800w,/assets/img/optimalTransport/equation/1_9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The Kantorovich problem is convex (the constrains are convex and one usually has that the cost functon \(c(x,y) = d(x-y)\) where d is convex). And we can also define the Kantorovich problem in discrete measures.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_10-480.webp 480w,/assets/img/optimalTransport/equation/1_10-800.webp 800w,/assets/img/optimalTransport/equation/1_10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And this is a linear programme. So Kantorovich is considered as the inventor of linear programming.</p> <h3 id="wasserstein-distance">Wasserstein Distance</h3> <p>When we talk about cost, it is always Eulerian based cost, such as \(L^P\), which defines a metric based on “pointwise differences”. However, this cost has some disadvantages. In the optimal transportation process, the probabilities between two points correspond to varying probabilities. Suppose the Euclidean distance is directly used to calculate the loss of transportation (or to measure and evaluate the transportation process). In that case, it will lead to a significant bias in the final evaluation (i.e., directly ignoring the definition of the probability vectors of the original different points).</p> <p>Optimal transport can be understood as a canonical way to lift a ground distance between points to a distance between historgram or measures. In order to evaluate the goodness of mapped paths for optimal transportation choices, the Wasserstein distance was developed.</p> <p>To give a clearer idea of what the Wasserstein distance does, suppose the three functions f,g, and h are in the figure below. If you were to represent the distance between f and g and the distance between f and h with L-infinity norm, you would find that the distance between f and g and between f and h are almost the same (\(\left \| f-g \right \|_{\infty } \approx \left \| f-h \right \|_{\infty }\)). However, under the Wasserstein distance, the distance between f and h is much larger than between f and g (\(d_{w_1}(f,q) &lt; d_{w_1} (f,h)\)).</p> <p>The \(f, g, h\) functions on \(R\):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/fgh-480.webp 480w,/assets/img/optimalTransport/fgh-800.webp 800w,/assets/img/optimalTransport/fgh-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/fgh.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>You can also see the difference between L-infinity norm and Wasserstein distance from the geodesics in the space of functions. If I want to move the function f to the function h, the changes of geodesics in figures below are totally different.</p> <p>The geodesics under L-infinity norm:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/normGeodesics-480.webp 480w,/assets/img/optimalTransport/normGeodesics-800.webp 800w,/assets/img/optimalTransport/normGeodesics-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/normGeodesics.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The geodesics under Wasserstein distance:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/wGeodesics-480.webp 480w,/assets/img/optimalTransport/wGeodesics-800.webp 800w,/assets/img/optimalTransport/wGeodesics-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/wGeodesics.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If we define Wasserstein distance based on the space of probability measures on \(X \subset \mathbb{R}^d\) with bounded \(p^{th}\) moment, i.e.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_11-480.webp 480w,/assets/img/optimalTransport/equation/1_11-800.webp 800w,/assets/img/optimalTransport/equation/1_11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If \(X\) is bounded then \(\mathcal{P}_p(X)=\mathcal{P}(X)\). The Wasserstein distance can be define as</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_12-480.webp 480w,/assets/img/optimalTransport/equation/1_12-800.webp 800w,/assets/img/optimalTransport/equation/1_12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And the wasserstein distance also satisfies the positive definiteness and triangular inequalities of the fugitive space.</p> <h2 id="duality">Duality</h2> <p>We saw in the previous chapter how Kantorovich’s optimal transport problem resembles a linear programme. It should not therefore be surprising that Kantorovich’s optimal transport problem admits a dual formulation.</p> <p>In optimization theory, turn original optimizing problem into dual problem can easily make it more easier to solve. Regardless of the difficulty of the original problem, dual problems are convex, and convex problems are a class of problems that are relatively easy to solve. When the original problem is a particularly difficult one, it is relatively easy to solve by reducing it to a dyadic problem</p> <h3 id="reminder-of-dual-problem">Reminder of Dual Problem</h3> <p>All optimization problems, in theory, can be transformed into standard form:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/dual1-480.webp 480w,/assets/img/optimalTransport/equation/dual1-800.webp 800w,/assets/img/optimalTransport/equation/dual1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/dual1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And then we define Lagrangian function:</p> \[L(x, \lambda, \mu) = f_0(x) + \sum_{i=1}^{m} \lambda_i f_i(x) + \sum_{j=1}^{p} \mu_j h_j(x)\] <p>With the Lagrangian function, we introduce the Lagrange dual function:</p> \[g(\lambda, \mu) = \inf_{x \in \mathcal{D}} L(x, \lambda, \mu)\] <p>The Lagrange dual function is the Lagrangian function that minimizes with respect to x. And we get the (Lagrangian) dual problem from the standard problem:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/dual2-480.webp 480w,/assets/img/optimalTransport/equation/dual2-800.webp 800w,/assets/img/optimalTransport/equation/dual2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/dual2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="kantorovich-duality">Kantorovich Duality</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/2_1-480.webp 480w,/assets/img/optimalTransport/equation/2_1-800.webp 800w,/assets/img/optimalTransport/equation/2_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/2_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Let \(\Phi_c\) defined by</p> \[\Phi_c = \{(\varphi, \psi) \in L^1(\mu) \times L^1(\nu) : \varphi(x) + \psi(y) \leq c(x, y)\}\] <p>where the inequality is understood to hold for \(\mu\)-almost every \(x \in X\) and \(\nu\)-almost every \(y \in Y\). Then,</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/2_2-480.webp 480w,/assets/img/optimalTransport/equation/2_2-800.webp 800w,/assets/img/optimalTransport/equation/2_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/2_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And if we look at the discrete optimal transport problem,</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/dual3-480.webp 480w,/assets/img/optimalTransport/equation/dual3-800.webp 800w,/assets/img/optimalTransport/equation/dual3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/dual3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>its dual problem is</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/2_3-480.webp 480w,/assets/img/optimalTransport/equation/2_3-800.webp 800w,/assets/img/optimalTransport/equation/2_3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/2_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="optimal-transport"/><category term="math"/><category term="optimalTransport"/><summary type="html"><![CDATA[this is an introduction about Optimal Transport]]></summary></entry></feed>