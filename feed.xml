<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ziye2chen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ziye2chen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-05T18:27:35+00:00</updated><id>https://ziye2chen.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The detail about Attention structure</title><link href="https://ziye2chen.github.io/blog/2024/attention/" rel="alternate" type="text/html" title="The detail about Attention structure"/><published>2024-05-23T00:01:00+00:00</published><updated>2024-05-23T00:01:00+00:00</updated><id>https://ziye2chen.github.io/blog/2024/attention</id><content type="html" xml:base="https://ziye2chen.github.io/blog/2024/attention/"><![CDATA[<h1 id="multi-head-attention">Multi-Head Attention</h1> <h2 id="scaled-dot-product-attention-and-multi-head-attention">Scaled Dot-Product Attention and Multi-Head Attention</h2> <p>The attention architecture derives from the mechanisms of human attention. When an image is placed in front of a human, the human scans the global image to obtain areas that are worth focusing on, and devotes more attention to these areas to obtain information. Nowadays, the popular attention model generally relies on the encoder-decoder framework, which can deal with tasks including NLP, image processing, etc.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/scaledAndMultiAttention-480.webp 480w,/assets/img/attention/scaledAndMultiAttention-800.webp 800w,/assets/img/attention/scaledAndMultiAttention-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/scaledAndMultiAttention.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Scaled Dot-Product Attention is the most basic attention structure. Multi-Head Attention is multiple parallel Scaled Dot-Product Attention, which splices the output of each Scaled Dot-Product Attention and does a linear transformation to output.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/transform-480.webp 480w,/assets/img/attention/transform-800.webp 800w,/assets/img/attention/transform-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/transform.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Attention can be considered as a specific word weighting, given a sequence of inputs \(\{x_1, x_2, \dots, x_n\}\), after the attentions layer, combining the input individuals \(\{x_1, x_2, \dots, x_n\}\) and outputs \(\{y_1, y_2, \dots, y_n\}\) will be as follows:</p> <ol> <li>The input features are multiplied by three sets of weight matrices \(W^Q, W^K, W^V\) to generate the three matrices of query, key, and value;</li> <li>The attention weight matrix \(A\) is obtained from the product of a certain \(Q\) and \(K\), which is normalized to get \(\hat{A}\);</li> <li>The normalized weights \(\hat{A}\) are multiplied by V to get the final output feature \(O\).</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/qkv-480.webp 480w,/assets/img/attention/qkv-800.webp 800w,/assets/img/attention/qkv-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/qkv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="detail-about-the-code-of-multi-head-attention">Detail about the code of Multi-Head Attention</h2> <p>At the beginning, we need to import some libraries we will normally use.</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">import torch
from torch import nn
import torch.functional as F
import math
</span></code></pre></div></div> <p>We need some data for testing. The dimension of testing data is 3.</p> <ul> <li>The first dimension represents batch.</li> <li>The second dimension represents time.</li> <li>The third dimension represnts the dimension after Encoder.</li> </ul> <p>And in language model, the ‘512’ below normally means the dimensions of the word vector do you want to map your word to after Embedding.</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">X = torch.randn(128,64,512) #Batch, Time, Dimension
print(X.shape)
</span></code></pre></div></div> <p>Next, we set up the basic parameters of Multihead attention.</p> <ul> <li>d_model represents the number of dimensions I want to map to the QKV space.</li> <li>n_head repersents the number of head.</li> </ul> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">d_model = 512
n_head = 8
</span></code></pre></div></div> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">class multi_head_attention(nn.Module):
</span># When we are writing a pytorch class, we need to inherit nn.Module
    def __init__(self, d_model, n_head) -&gt; None:
        super(multi_head_attention, self).__init__() # Initialize some basic parameters
<span class="err">
</span>        self.d_model = d_model
        self.n_head = n_head
<span class="err">
</span>        self.w_q = nn.Linear(d_model, d_model) #Query
        self.w_k = nn.Linear(d_model, d_model) #Key
        self.w_v = nn.Linear(d_model, d_model) #Value
        # It's like we are looking for some queries to match some keys, and values are then weighted to combine.
<span class="err">
</span>        self.w_combine = nn.Linear(d_model, d_model)
        # Because of 'multi-head', we need a combinatorial mapping of the outputs.
<span class="err">
</span>        self.softmax = nn.Softmax(dim=-1)
</code></pre></div></div> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#Still in class multi_head_attention(nn.Module):
    def forward(self, q, k, v, mask = None):
        batch, time, dimension = q.shape # get the dimensions of Q, K, V
        n_d = self.d_model // self.n_head # Because we have n heads, 'd_model' for each submodel has to be divisible by 'n_head'.
        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)
<span class="err">
</span>        q = q.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)   
        k = k.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)   
        v = v.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)   
        # We cannot put 'n_head' in the last dimension because we need to process the last two dimensions.
<span class="err">
</span>        score = q @ k.transpose(2, 3) / math.sqrt(n_d) # The most important code in Attention.
<span class="err">
</span>        minusInfty = -10000
        if mask is not None:
            score = score.masked_fill(mask == 0, minusInfty)
        score = self.softmax(score) @ v
<span class="err">
</span>        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension)
<span class="err">
</span>        output = self.w_combine(score)
        
        return output 
</code></pre></div></div> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">attention = multi_head_attention(d_model, n_head)
output = attention(X, X, X)
print(output, output.shape)
</span></code></pre></div></div> <h2 id="something-you-might-want-to-know-about-attention">Something you might want to know about Attention</h2>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is the detail about attention structure]]></summary></entry></feed>