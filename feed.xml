<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ziye2chen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ziye2chen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-26T19:40:56+00:00</updated><id>https://ziye2chen.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Introduction to KAN</title><link href="https://ziye2chen.github.io/blog/2024/kan/" rel="alternate" type="text/html" title="Introduction to KAN"/><published>2024-06-01T00:01:00+00:00</published><updated>2024-06-01T00:01:00+00:00</updated><id>https://ziye2chen.github.io/blog/2024/kan</id><content type="html" xml:base="https://ziye2chen.github.io/blog/2024/kan/"><![CDATA[<h2 id="kan-and-mlp">KAN and MLP</h2> <p>The emergence of the <a href="https://arxiv.org/pdf/2404.19756">KANs (Kolmogorov-Arnold Networks)</a> model has led to heated discussions in the AI community, and the reason for this is that the flaws in the structure of the MLPs itself have been troubling the researchers involved.</p> <p>Nowadays, most AI research is based on MLP. The appearance of KANs can potentially revolutionize the direction of AI research. If KANs are as marvelous as expected in subsequent research, their appearance will be like the emergence of cement in a society that has been only using wood to build houses.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/2_1-480.webp 480w,/assets/img/KAN/2_1-800.webp 800w,/assets/img/KAN/2_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/2_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="the-drawbacks-of-mlp">The drawbacks of MLP</h2> <p>The theoretical basis for MLPs is that a single hidden layer network containing enough neurons can approximate any continuous function. But MLPs have some natural flaws:</p> <h3 id="vanishing-and-exploding-gradients">Vanishing and exploding gradients</h3> <p><strong><em>Vanishing gradients:</em></strong> the gradient tends to zero, the network weights cannot be updated or are updated very slightly, and the network will not be effective even if it is trained for a long time;</p> <p><strong><em>Exploding gradient:</em></strong> the gradient grows exponentially and becomes very large, leading to a large update of the network weights, making the network unstable.</p> <p>Whether the gradient disappears or the gradient explosion is essentially due to the backpropagation of the deep neural network.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/NN-480.webp 480w,/assets/img/KAN/NN-800.webp 800w,/assets/img/KAN/NN-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/NN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For example, in a four-layer neural network, its backpropagation formula is as follows:</p> \[\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial y_4} \frac{\partial y_4}{\partial z_4} \frac{\partial z_4}{\partial y_3} \frac{\partial y_3}{\partial z_3} \frac{\partial z_3}{\partial y_2} \frac{\partial y_2}{\partial z_2} \frac{\partial z_2}{\partial y_1} \frac{\partial y_1}{\partial z_1} \frac{\partial z_1}{\partial w_1}\] \[\frac{\partial L}{\partial y_4} = \sigma'(z_4) w_4 \sigma'(z_3) w_3 \sigma'(z_2) w_2 \sigma'(z_1) x_1\] <p>where \(y_i = \sigma(z_i) = \sigma(w_i x_i + b_i)\), which \(x_i = y_{i-1}\), and \(\sigma\) is <strong><em>sigmoid</em></strong> activation function.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/sigmoid-480.webp 480w,/assets/img/KAN/sigmoid-800.webp 800w,/assets/img/KAN/sigmoid-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/sigmoid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>From the above, it can be seen that there is a sequence of multiplications in the backpropagation of the neural network. Also, the derivative of the sigmoid activation function has a value range of 0 to 0.25. So when each \(w_i\) is small, and there are many \(\sigma'\) multiplied together, the gradient tends to 0. When each \(w_i\) very large, the gradient becomes very large.</p> <p>While many methods say they solve this problem, the issue is still complicated to avoid completely, as it is an intrinsic shortcoming of MLPs.</p> <h3 id="inefficient-utilization-of-parameters">Inefficient utilization of parameters</h3> <p>MLPs usually use fully connected networks, so the number of parameters is huge. When the network becomes very deep, only a few parameters are utilized. And this is one reason why LLM may have reached a dead end.</p> <p>Although it is possible to simplify the structure using CNNs or regularization or the like, essentially, the structure of a highly dense continuous linear model coupled with an activation function dictates that the MLP can’t be too sparse or will not have enough representational power.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/cnn-480.webp 480w,/assets/img/KAN/cnn-800.webp 800w,/assets/img/KAN/cnn-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/cnn.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="inadequate-capacity-to-handle-high-dimensional-data-and-long-term-dependency-issues">Inadequate capacity to handle high-dimensional data and Long-term dependency issues</h3> <p>MLPs do not utilize the intrinsic structure of the data, such as local spatial correlations in images and sequence information in textual data. Although CNN and RNN can improve these problems, this base module of MLPs determines that these problems cannot be solved completely.</p> <p>MLPs also have difficulty capturing long-standing relationships in input sequences. (RNN &amp; Transformer can improve it)</p> <h2 id="kolmogorov-arnold-networks">Kolmogorov-Arnold Networks</h2> <h3 id="the-kolmogorov-arnold-representation-theorem">The Kolmogorov-Arnold representation theorem</h3> <p>KANs are inspired by the Kolmogorov-Arnold representation theorem. Vladimir Arnold and Andrey Kolmogorov established that if f is a multivariate continuous function on a bounded domain, then f can be written as a finite composition of continuous functions of a single variable and the binary operation of addition.</p> <p>More specifically, for a smooth \(f : [0, 1]^n \rightarrow \mathbb{R}\),</p> \[f(\mathbf{x}) = f(x_1, \dots, x_n) = \sum_{q=1}^{2n+1} \Phi_q \left( \sum_{p=1}^n \phi_{q,p}(x_p) \right)\] <p>where \(\phi_{q,p} : [0,1] \rightarrow \mathbb{R}\) and \(\Phi_q : \mathbb{R} \rightarrow \mathbb{R}\). This theorem reveals how any multivariate continuous function can be represented by a simpler set of functions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/2_2-480.webp 480w,/assets/img/KAN/2_2-800.webp 800w,/assets/img/KAN/2_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/2_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It can be shown that the Kolmogorov-Arnold representation theorem is a simple KANs which has only two-layer nonlinearities and a small number of terms (2n + 1) in the hidden layer. This structure appeared a long time ago, but the difficulty is how to make the KANs deeper. And two-layer networks are too simple to approximate complex functions. (Details of deepening KANs will be mentioned later)</p> <h3 id="why-kans-are-awesome">Why KANs are awesome</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/0_1-480.webp 480w,/assets/img/KAN/0_1-800.webp 800w,/assets/img/KAN/0_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/0_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>KANs contrast MLPs by not having a nested relationship between activation functions and parameter matrices but rather a nesting of directly nonlinear functions \(\Phi\). And \(\Phi\) all use the same function structure. (In the paper, they use the spline function in numerical analysis)</p> \[KAN(x) = (\Phi_{L-1} \circ \Phi_{L-2} \circ \cdots \circ \Phi_1 \circ \Phi_0)x\] \[MLP(x) = (W_{L-1} \circ \sigma \circ W_{L-2} \circ \sigma \circ \cdots \circ W_1 \circ \sigma \circ W_0)x\] <p>When learning the network parameters, MLPs learn the linear function with a fixed nonlinear activation function. In contrast, KANs learn the parameterized nonlinear function directly, which enhances their characterization ability.</p> <p>Because of the parameters’ complexity, although individual spline functions are more complex to learn than linear functions, KANs typically allow for smaller computational graphs than MLPs (i.e., a smaller network size is required to achieve the same effect).</p> <h3 id="the-details-about-kans">The details about KANs</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/spline-480.webp 480w,/assets/img/KAN/spline-800.webp 800w,/assets/img/KAN/spline-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/spline.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If MLPs fit a function with some lines, KANs fit a function with some curves. In KANs, the spline function is used as the nonlinear function \(\Phi\).</p> <p>The KAN layer is simply a matrix of functions \(\Phi = \{ \phi_{q,p} \}\), \(p=1,2,\cdots,n_{\text{in}}\), $$q=1,2,\cdots,n_{\text{out}}.</p> \[\Phi_l = \left( \begin{array}{ccc} \phi_{l,1,1}(\cdot) &amp; \phi_{l,1,2}(\cdot) &amp; \cdots &amp; \phi_{l,1,n_l}(\cdot) \\ \phi_{l,2,1}(\cdot) &amp; \phi_{l,2,2}(\cdot) &amp; \cdots &amp; \phi_{l,2,n_l}(\cdot) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \phi_{l,n_{l+1},1}(\cdot) &amp; \phi_{l,n_{l+1},2}(\cdot) &amp; \cdots &amp; \phi_{l,n_{l+1},n_l}(\cdot) \end{array} \right)\] \[X_{l+1} = \Phi_l X_l\] <p>where \(\Phi_l\) is the function matrix corresponding to the \(l_{\text{th}}\) KAN layer.</p> <p>Although the KAN layer looks very simple, it is not easy to make it well optimizable. The key tricks are as follows:</p> <h4 id="1-residual-activation-functions">1. Residual activation functions：</h4> \[\phi(x) = w \big(b(x) + \text{spline}(x)\big)\] <p>\(b(x)\) is a basis function,</p> \[b(x) = \text{SiLU}(x) = \frac{x}{1 + e^{-x}}\] <p>SiLU (Sigmoid Linear Unit) function is an improved version of Sigmoid and ReLU.SiLU has the properties of having no upper and lower bounds, and it is smooth and non-monotonic.SiLU works better than ReLU on deep models. It can be seen as a smooth ReLU activation function.SiLU activation function is also known as the Swish activation function, an adaptive activation function introduced by Google Brain in 2017.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/SiLU-480.webp 480w,/assets/img/KAN/SiLU-800.webp 800w,/assets/img/KAN/SiLU-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/SiLU.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>\(\text{spline}(x)\) is parametrized as a linear combination of B-splines such that</p> \[\text{spline}(x) = \sum_i c_i B_i(x)\] <p>where every \(c_i\) is trainable. The B-spline function \(\text{spline}(x)\) can be considered a linear combination of control points \(c_i\) and a weighting function \(B_i(x)\). In short, the B-spline curve is the weighted sum of each control point multiplied by its corresponding weight function. The weighting function is predefined and is only related to order D, and it does not change as the control points change.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KAN/bSpline-480.webp 480w,/assets/img/KAN/bSpline-800.webp 800w,/assets/img/KAN/bSpline-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/KAN/bSpline.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In principle \(w\) is redundant since it can be absorbed into \(b(x)\) and \(\text{spline}(x)\). However \(w\) can better control the overall magnitude of the activation function.</p> <h4 id="2-initialization-scales">2. Initialization scales:</h4> <p>Each activation function is initialized to have \(\text{spline}(x) \approx 0\).</p> <p>\(w\) is initialized according to Xavier initialization. Xavier initialization is a very effective method for initializing neural networks, and the method originates from a 2010 paper, <a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a>.</p> <p>In the paper, Xavier Glorot offers the insight that the variance of the activation values decreases layer by layer, which results in the gradient in backpropagation and also decreases layer by layer. The solution to the vanishing gradient is to avoid the decay of the variance of the activation values and, ideally, to keep the output values (activation values) of each layer Gaussian distributed. So they initialized the weights \(W\) at each layer with the following commonly used heuristic:</p> \[W \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_j + n_{j+1}}}, \frac{\sqrt{6}}{\sqrt{n_j + n_{j+1}}}\right]\] <p>where \(n_j\) is the number of inputs, and \(n_{j+1}\) is the number of outputs.</p> <p>In addition, each KAN layer has an equal layer width, with L layers and N nodes per layer.</p> <p>Each spline function is typically of order k = 3, G intervals, and G + 1 grid points.</p> <p>Thus KANs require a total of \(O(N^2L(G+k))\) or \(O(N^2LG)\) parameters, while MLPs require \(O(N^2L)\) parameters. It may seem that the computational complexity of KANs is greater than that of MLPs, but KANs require much less \(N\) than MLPs, which not only saves parameters but also improves generalization and aids interpretability.</p> <h4 id="3-update-of-spline-grids">3. Update of spline grids:</h4> <p>In KANs, they update each grid on the fly according to its input activations, to address the issue that splines are defined on bounded regions but activation values can evolve out of the fixed region during training.</p>]]></content><author><name></name></author><category term="AI"/><category term="structure"/><category term="paperReading"/><summary type="html"><![CDATA[this is an introduction about KAN]]></summary></entry><entry><title type="html">The detail about Attention structure</title><link href="https://ziye2chen.github.io/blog/2024/attention/" rel="alternate" type="text/html" title="The detail about Attention structure"/><published>2024-05-23T00:01:00+00:00</published><updated>2024-05-23T00:01:00+00:00</updated><id>https://ziye2chen.github.io/blog/2024/attention</id><content type="html" xml:base="https://ziye2chen.github.io/blog/2024/attention/"><![CDATA[<h2 id="attention-structure">Attention Structure</h2> <h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3> <p>The attention architecture derives from the mechanisms of human attention. When an image is placed in front of a human, the human scans the global image to obtain areas that are worth focusing on, and devotes more attention to these areas to obtain information. Nowadays, the popular attention model generally relies on the encoder-decoder framework, which can deal with tasks including NLP, image processing, etc.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/scaledAndMultiAttention-480.webp 480w,/assets/img/attention/scaledAndMultiAttention-800.webp 800w,/assets/img/attention/scaledAndMultiAttention-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/scaledAndMultiAttention.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Scaled Dot-Product Attention is the most basic attention structure. Multi-Head Attention is multiple parallel Scaled Dot-Product Attention, which splices the output of each Scaled Dot-Product Attention and does a linear transformation to output.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/transform-480.webp 480w,/assets/img/attention/transform-800.webp 800w,/assets/img/attention/transform-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/transform.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Attention can be considered as a specific word weighting, given a sequence of inputs \(\{x_1, x_2, \dots, x_n\}\), after the attentions layer, combining the input individuals \(\{x_1, x_2, \dots, x_n\}\) and outputs \(\{y_1, y_2, \dots, y_n\}\) will be as follows:</p> <ol> <li>The input features are multiplied by three sets of weight matrices \(W^Q, W^K, W^V\) to generate the three matrices of query, key, and value;</li> <li>The attention weight matrix \(A\) is obtained from the product of a certain \(Q\) and \(K\), which is normalized to get \(\hat{A}\);</li> <li>The normalized weights \(\hat{A}\) are multiplied by V to get the final output feature \(O\).</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/qkv-480.webp 480w,/assets/img/attention/qkv-800.webp 800w,/assets/img/attention/qkv-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/qkv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Mathematically, the self-attention matrix for input matrices \((Q, K, V)\) is calculated as:</p> \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/Y-480.webp 480w,/assets/img/attention/Y-800.webp 800w,/assets/img/attention/Y-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/Y.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="multi-head-attention">Multi-Head Attention</h3> <p>If each vector \(q_i, k_i, v_i\) is split into n, we can get the n-headed attention mechanism. It is recognized that multi-head self-attention mechanisms are better than single-head ones because it can capture information in more dimensions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention/multihead-480.webp 480w,/assets/img/attention/multihead-800.webp 800w,/assets/img/attention/multihead-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention/multihead.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>At the beginning, we need to import some libraries we will normally use.</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">import torch
from torch import nn
import torch.functional as F
import math
</span></code></pre></div></div> <p>We need some data for testing. The dimension of testing data is 3.</p> <ul> <li>The first dimension represents batch.</li> <li>The second dimension represents time.</li> <li>The third dimension represnts the dimension after Encoder.</li> </ul> <p>And in language model, the ‘512’ below normally means the dimensions of the word vector do you want to map your word to after Embedding.</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">X = torch.randn(128,64,512) #Batch, Time, Dimension
print(X.shape)
</span></code></pre></div></div> <p>Next, we set up the basic parameters of Multihead attention.</p> <ul> <li>d_model represents the number of dimensions I want to map to the QKV space.</li> <li>n_head repersents the number of head.</li> </ul> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">d_model = 512
n_head = 8
</span></code></pre></div></div> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">class multi_head_attention(nn.Module):
</span># When we are writing a pytorch class, we need to inherit nn.Module
    def __init__(self, d_model, n_head) -&gt; None:
        super(multi_head_attention, self).__init__() # Initialize some basic parameters
<span class="err">
</span>        self.d_model = d_model
        self.n_head = n_head
<span class="err">
</span>        self.w_q = nn.Linear(d_model, d_model) #Query
        self.w_k = nn.Linear(d_model, d_model) #Key
        self.w_v = nn.Linear(d_model, d_model) #Value
        # It's like we are looking for some queries to match some keys, and values are then weighted to combine.
<span class="err">
</span>        self.w_combine = nn.Linear(d_model, d_model)
        # Because of 'multi-head', we need a combinatorial mapping of the outputs.
<span class="err">
</span>        self.softmax = nn.Softmax(dim=-1)
</code></pre></div></div> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#Still in class multi_head_attention(nn.Module):
    def forward(self, q, k, v, mask = None):
        batch, time, dimension = q.shape # get the dimensions of Q, K, V
        n_d = self.d_model // self.n_head # Because we have n heads, 'd_model' for each submodel has to be divisible by 'n_head'.
        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)
<span class="err">
</span>        q = q.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)   
        k = k.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)   
        v = v.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)   
        # We cannot put 'n_head' in the last dimension because we need to process the last two dimensions.
<span class="err">
</span>        score = q @ k.transpose(2, 3) / math.sqrt(n_d) # The most important code in Attention.
<span class="err">
</span>        minusInfty = -10000
        if mask is not None:
            score = score.masked_fill(mask == 0, minusInfty)
        score = self.softmax(score) @ v
<span class="err">
</span>        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension)
<span class="err">
</span>        output = self.w_combine(score)
        
        return output 
</code></pre></div></div> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">attention = multi_head_attention(d_model, n_head)
output = attention(X, X, X)
print(output, output.shape)
</span></code></pre></div></div> <h2 id="something-you-might-want-to-know-about-attention">Something you might want to know about Attention</h2> <h3 id="the-square-root-of-d-in-the-attention-calculation">The square root of d in the Attention calculation</h3> <p><strong>Why should we divide \(\sqrt{d}\) in the Attention calculation?</strong></p> <p>Assume that the Query vector \(q\) and the Key vector \(k\) have dimension \(d\) and their elements are independent and identically distributed random variables with mean \(0\) and variance \(1\).</p> <p>Then their dot product \(q \cdot k\) can be expressed as: \(q \cdot k = \sum_{i=1}^d q_i k_i\). Since each \(q_i\) and \(k_i\) is an independently and identically distributed random variable, their product \(q_i k_i\) is also a random variable. For each \(q_i k_i\), the expectation is \(0\) and the variance is \(1\).</p> <p>Therefore, the expected value of the dot product \(q \cdot k\) is:</p> \[\mathbb{E}[q \cdot k] = \mathbb{E} \left[ \sum_{i=1}^d q_i k_i \right] = \sum_{i=1}^d \mathbb{E}[q_i k_i] = 0\] <p>Moreover, its variance is:</p> \[\text{Var}(q \cdot k) = \text{Var} \left( \sum_{i=1}^d q_i k_i \right) = \sum_{i=1}^d \text{Var}(q_i k_i) = d\] <p>Thus, the standard deviation of the dot product \(q \cdot k\) is:</p> \[\sigma = \sqrt{\text{Var}(q \cdot k)} = \sqrt{d}\] <p>By the Central Limit Theorem, the sum of these independently and identically distributed random variables will converge to a normal distribution when $d$ is large enough. To normalize this, we subtract the expected value and divide by the standard deviation:</p> \[\frac{q \cdot k - \mathbb{E}[q \cdot k]}{\sqrt{\text{Var}(q \cdot k)}}\] <p>Here, \(\mathbb{E}[q \cdot k] = 0\) and \(\text{Var}(q \cdot k) = d\), so the expression becomes:</p> \[\frac{q \cdot k}{\sqrt{d}}\] <p>This will make the expression approximately obey the standard normal distribution \(N(0, 1)\).</p>]]></content><author><name></name></author><category term="AI"/><category term="structure"/><category term="code"/><summary type="html"><![CDATA[this is the detail about attention structure]]></summary></entry><entry><title type="html">Modern Solution of Optimal Transport - Sinkhorn</title><link href="https://ziye2chen.github.io/blog/2024/sinkhorn/" rel="alternate" type="text/html" title="Modern Solution of Optimal Transport - Sinkhorn"/><published>2024-03-30T00:01:00+00:00</published><updated>2024-03-30T00:01:00+00:00</updated><id>https://ziye2chen.github.io/blog/2024/sinkhorn</id><content type="html" xml:base="https://ziye2chen.github.io/blog/2024/sinkhorn/"><![CDATA[<h2 id="modern-solution-of-optimal-transport---sinkhorn">Modern Solution of Optimal Transport - Sinkhorn</h2> <p>In 2013, Cuturi proposed a scalable approximation of optimal transport. Since then, OT is increasingly becoming a core tool of machine learning research toolbox.</p> <p>The computation of other optimal transport formulation involves the resolution of a linear program whose cost can quickly become prohibitive.</p> <p>But Sinkhorn distance looks at transport problems from a maximum-entropy perspective. It smooths the classic optimal transport problem with an entropic regularization term. And the computation through Sinkhorn’s matrix scaling algorithm is several orders of magnitude faster than that of transport solvers, which is \(O(n^2)\).</p> <h3 id="sinkhorn-distance">Sinkhorn Distance</h3> <p>In what follows, \(\left \langle \cdot , \cdot \right \rangle\) stands for the Frobenius dot-product. For two probability vectors \(\mu\) and \(\nu\) in the simplex \(\sum_d := \{ x \in \mathbb{R}_+^{d} : x^T \mathbf{1}_d=1 \}\), where \(\mathbf{1}_d\) is the d dimensional vector of ones, we write \(U(r,c)\) for the transport polytope of \(\mu\) and \(\nu\), namely the polyhedral set of \(d \times d\) matrices,</p> \[U(\mu,\nu) := \{ P \in \mathbb{R}^{d\times d}_+ | P \mathbf{1}_d = \mu, P^T \mathbf{1}_d = \nu \}\] <p>And we define the entropy $h$ and Kullback-Leibler divergences of \(P,Q\in U(\mu,\nu)\) and a marginals \(\mu \in \sum_d\) as</p> \[h(\mu) = -\sum_{i=1}^{d}\mu_i log \mu_i,\quad h(P) = -\sum_{i,j=1}^{d}p_{ij} log p_{ij}, \quad \mathbf{KL}(P||Q)=\sum_{ij}p_{ij} log\frac{p_{ij}}{q_{ij}}\] <p>where \(p(X=i,Y=j)=p_{ij}\) and the same as \(q_ij\).</p> <p>The following information theoretic inequality (Cover and Thomas, 1991, §2) for joint probabilities</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/3_1-480.webp 480w,/assets/img/optimalTransport/equation/3_1-800.webp 800w,/assets/img/optimalTransport/equation/3_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/3_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>is tight, since the independence table \(\mu\nu^T\) has entropy \(h(\mu\nu^T) = h(\mu)+h(\nu)\). By the concavity of entropy, we can introduce the convex set</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/3_2_3_3-480.webp 480w,/assets/img/optimalTransport/equation/3_2_3_3-800.webp 800w,/assets/img/optimalTransport/equation/3_2_3_3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/3_2_3_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <table> <tbody> <tr> <td>These two definitions are indeed equivalent, since one can easily check that $$\mathbf{KL}(P</td> <td> </td> <td>\mu\nu^T)= h(\mu)+h(\nu)-h(P)$$, a quantity which is also the mutual information $I(X</td> <td> </td> <td>Y)$ of two random variables \((X,Y)\) should they follow the joint probability \(P\) (Cover and Thomas, 1991, §2).</td> </tr> </tbody> </table> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/3_4-480.webp 480w,/assets/img/optimalTransport/equation/3_4-800.webp 800w,/assets/img/optimalTransport/equation/3_4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/3_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Why consider an entropic constraint in optimal transport? The first reason is computational. The second reason is built upon the following intuition. As a classic result of linear optimization, the OT problem is always solved on a vertex of \(U(\mu,\nu)\). Such a vertex is a sparse \(d\times d\) matrix with only up to \(2d-1\) non-zero elements (Brualdi, 2006, §8.1.3).</p> <p>When α is large enough, the Sinkhorn distance coincides with the classic OT distance. When α = 0, the Sinkhorn distance has a closed form and becomes a negative definite kernel if one assumes that M is itself a negative definite distance, or equivalently a Euclidean distance matrix.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/3_5-480.webp 480w,/assets/img/optimalTransport/equation/3_5-800.webp 800w,/assets/img/optimalTransport/equation/3_5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/3_5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/3_6-480.webp 480w,/assets/img/optimalTransport/equation/3_6-800.webp 800w,/assets/img/optimalTransport/equation/3_6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/3_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/3_7-480.webp 480w,/assets/img/optimalTransport/equation/3_7-800.webp 800w,/assets/img/optimalTransport/equation/3_7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/3_7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And \(d_{M,\alpha}(x,y)\) also satisfies the triangle inequality:</p> \[d_{M,\alpha}(x,z) \le d_{M,\alpha}(x,y)+d_{M,\alpha}(y,z)\] <h3 id="computing-regularized-transport-with-sinkhorns-algorithm">Computing Regularized Transport with Sinkhorn’s Algorithm</h3> <p>We consider in this section a Lagrange multiplier for the entropy constraint of Sinkhorn distances:</p> \[For \lambda &gt; 0, d_{M}^{\lambda}(\mu, \nu) := \langle P^{\lambda}, M \rangle, \text{where} \ P^{\lambda} = \underset{P \in U(\mu,\nu)}{\text{argmin}} \langle P, M \rangle - \frac{1}{\lambda}h(P).\] <p>By duality theory we have that to each \(\alpha\) corresponds a \(\lambda \in [0,\infty]\) such that \(d_{M,\alpha}(\mu, \nu)=d_{M}^{\lambda}(\mu, \nu)\) holds for that pair \((\mu,\nu)\). We call \(d_{M}^{\lambda}\) the dual-Sinkhorn divergence and show that it can be computed for a much cheaper cost than the original distance \(d_M\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/sinkhorn-480.webp 480w,/assets/img/optimalTransport/sinkhorn-800.webp 800w,/assets/img/optimalTransport/sinkhorn-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/sinkhorn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="pytorch-implementation-of-sinkhorn">PyTorch implementation of sinkhorn</h3> <p><a href="https://github.com/vlkit/vlkit/blob/main/vlkit/optimal_transport/sinkhorn.py">vlkit.optimal_transport.sinkhorn</a> has a PyTorch implementation of sinkhorn that allows us to compute and visualize the optimal transport between two distributions. As an example, two 1d Gaussian distributions are generated as source and target distributions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">matplotlib</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">gridspec</span>
<span class="kn">from</span> <span class="n">vlkit.optimal_transport</span> <span class="kn">import</span> <span class="n">sinkhorn</span>

<span class="c1"># generate two gaussians as the source and target
</span><span class="k">def</span> <span class="nf">gaussian</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">std</span><span class="o">**</span><span class="mi">2</span><span class="p">)).</span><span class="nf">exp</span><span class="p">()</span>
    <span class="n">d</span> <span class="o">/=</span> <span class="n">d</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">d</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">d1</span> <span class="o">=</span> <span class="nf">gaussian</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">d2</span> <span class="o">=</span> <span class="nf">gaussian</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="n">dist</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">abs</span><span class="p">().</span><span class="nf">float</span><span class="p">()</span>
<span class="n">dist</span> <span class="o">/=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span>

<span class="c1"># visualize distr
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">bar</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">d1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Source distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">bar</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">d2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Target distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/distribution-480.webp 480w,/assets/img/optimalTransport/distribution-800.webp 800w,/assets/img/optimalTransport/distribution-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">T</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="nf">sinkhorn</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="n">d1</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="n">d2</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">reg</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="p">.</span><span class="nc">GridSpec</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">d2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Target distribution</span><span class="sh">'</span><span class="p">)</span>

<span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">barh</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">d1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Source distribution</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">invert_xaxis</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">invert_yaxis</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="n">sharex</span><span class="o">=</span><span class="n">ax1</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="n">ax2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">T</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/transport-480.webp 480w,/assets/img/optimalTransport/transport-800.webp 800w,/assets/img/optimalTransport/transport-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/transport.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="optimal-transport"/><category term="math"/><category term="code"/><category term="optimalTransport"/><summary type="html"><![CDATA[this is my study note about sinkhorn]]></summary></entry><entry><title type="html">Optimal Transport</title><link href="https://ziye2chen.github.io/blog/2024/optimal-transport/" rel="alternate" type="text/html" title="Optimal Transport"/><published>2024-03-23T00:01:00+00:00</published><updated>2024-03-23T00:01:00+00:00</updated><id>https://ziye2chen.github.io/blog/2024/optimal-transport</id><content type="html" xml:base="https://ziye2chen.github.io/blog/2024/optimal-transport/"><![CDATA[<h2 id="intro-to-optimal-transport">Intro to Optimal Transport</h2> <p>Optimal transport problem can be understood as a problem of two piles of soil, shoveled from soil A to the other and eventually piled up to form soil B. And your mission is to find the most efficient method to shovel the soil.</p> <p>There are two ways to formulate the optimal transport problem: the Monge and Kantorovich formulations. Historically the Monge formulation comes before Kantorovich. The Kantorovich formulation can be seen as a generalisation of Monge. Both formulations have their advantages and disadvantages. Monge is more useful in applications, whilst Kantorovich is more useful theoretically.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/soil-480.webp 480w,/assets/img/optimalTransport/soil-800.webp 800w,/assets/img/optimalTransport/soil-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/soil.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="measure">Measure</h3> <p>In mathematics, measure is a generalization and formalization of geometrical measures like length, area, volume, magnitude, mass and probability. In Lebesgue measure, for lower dimensions n = 1, 2, or 3, it coincides with the standard measure of length, area, or volume.</p> <p>The concept of measures will be used later, and before I introduce probability measures, I need to introduce the histogram. The histogram here is the probability of n sums of 1, representing a probability distribution. We will use interchangeably the terms histogram and probability vector for any element $a \in \Sigma_n $ that belongs to the probability simplex</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_1-480.webp 480w,/assets/img/optimalTransport/equation/1_1-800.webp 800w,/assets/img/optimalTransport/equation/1_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And we use discrete measure describes a probability measure if, additionally, \(a \in \Sigma_n\) and more generally a positive measure if all the elements of vector \(a\) are non-negative.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_2-480.webp 480w,/assets/img/optimalTransport/equation/1_2-800.webp 800w,/assets/img/optimalTransport/equation/1_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And to avoid degeneracy issues where locations with no mass are accounted for, we will assume when considering discrete measures that all the elements of \(a\) are positive.</p> <p>A convenient feature of OT is that it can deal with measures that are either or both discrete and continuous within the same framework. So we can find a relation between discrete measures and general measures.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_3-480.webp 480w,/assets/img/optimalTransport/equation/1_3-800.webp 800w,/assets/img/optimalTransport/equation/1_3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="the-monge-formulation">The Monge Formulation</h3> <p>In optimal transport, there are two measures \(\mu\) and \(\nu\). You can imagine they are two piles of soil (but I will call them measure for the rest). And you can also consider the first measure \(\mu\) as a pile of sand and the second measure \(\nu\) as a hole we wish to fill up. We assume that both measures are probability measures on spaces $X$ and $Y$ respectively. The cost function \(c(x,y)\), \(c:X \to [0,+\infty]\), measures the cost of transporting one unit of mass from \(x \in X\) to \(y \in Y\). Now, the problem is to find the most optimal method.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_4-480.webp 480w,/assets/img/optimalTransport/equation/1_4-800.webp 800w,/assets/img/optimalTransport/equation/1_4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For greter generality we work with the inverse of \(T\) itself. The inverse is treated in the general set valued sense, i.e. \(x \in T^{-1}(y)\) if \(T(x)=y\), if the function \(T\) is injective then we can equivalently say that \(\nu (T(A)) = \mu (A)\) for all \(\mu\)-measurable \(A\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/T-480.webp 480w,/assets/img/optimalTransport/T-800.webp 800w,/assets/img/optimalTransport/T-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/T.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_5_1_6-480.webp 480w,/assets/img/optimalTransport/equation/1_5_1_6-800.webp 800w,/assets/img/optimalTransport/equation/1_5_1_6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_5_1_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Transport map \(T\) does not always exist. For example, there are two discrete measures \(\mu = \delta_{x_1}\), \(\nu = \frac{1}{2} \delta_{y_1} + \frac{1}{2} \delta_{y_2}\), where \(y_1 \not= y_2\). Then \(\nu({y_1}) = \frac{1}{2}\) but \(\mu(T^{-1}(y_1)) \in \{0,1\}\) depending on whether \(x \in T^{-1}(y_1)\). Hence no transport maps exist.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_7-480.webp 480w,/assets/img/optimalTransport/equation/1_7-800.webp 800w,/assets/img/optimalTransport/equation/1_7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The original monge’s optimal transport problem used \(L^1\) cost, i.e. \(c(x,y) = \mid x - y \mid\). But it is much harder to solve than with \(L^2\) cost, i.e. \(c(x,y) = \mid x - y \mid^2\).</p> <p>If the measure is discrete measure, we can also define the Monge’s optimal transport problem between discrete measures.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_8-480.webp 480w,/assets/img/optimalTransport/equation/1_8-800.webp 800w,/assets/img/optimalTransport/equation/1_8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="the-kantorovich-formulation">The Kantorovich Formulation</h3> <p>In the example \(\mu = \delta_{x_1}\), \(\nu = \frac{1}{2} \delta_{y_1} + \frac{1}{2} \delta_{y_2}\), it causes problem in Monge formulation because mass cannot be split. To allow mass to be split, the Kantorovich formulation appears, which has a natural relaxation. To formalize this, we consider a measure \(\pi \in \mathcal{P}(X \times Y)\) and think of \(d\pi(x,y)\) as the amount of mass transferred from $x$ to $y$. And we have the constrains:</p> <p>\(\pi(A \times Y) = \mu (A)\), \(\pi(X \times B) = \nu (B)\) for all mesaurable sets \(A \subseteq X\), \(B \subseteq Y\).</p> <p>And we denote the set of such \(\pi\) by \(\Pi(\mu,\nu)\) and call \(\Pi(\mu,\nu)\) the set of transport plans between \(\mu\) and \(\nu\). \(\Pi(\mu,\nu)\) is never non-empty and \(\mu \otimes \nu \in \Pi(\mu,\nu)\). Now we can define Kantorovich’s formulation of optimal transport.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_9-480.webp 480w,/assets/img/optimalTransport/equation/1_9-800.webp 800w,/assets/img/optimalTransport/equation/1_9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The Kantorovich problem is convex (the constrains are convex and one usually has that the cost functon \(c(x,y) = d(x-y)\) where d is convex). And we can also define the Kantorovich problem in discrete measures.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_10-480.webp 480w,/assets/img/optimalTransport/equation/1_10-800.webp 800w,/assets/img/optimalTransport/equation/1_10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And this is a linear programme. So Kantorovich is considered as the inventor of linear programming.</p> <h3 id="wasserstein-distance">Wasserstein Distance</h3> <p>When we talk about cost, it is always Eulerian based cost, such as \(L^P\), which defines a metric based on “pointwise differences”. However, this cost has some disadvantages. In the optimal transportation process, the probabilities between two points correspond to varying probabilities. Suppose the Euclidean distance is directly used to calculate the loss of transportation (or to measure and evaluate the transportation process). In that case, it will lead to a significant bias in the final evaluation (i.e., directly ignoring the definition of the probability vectors of the original different points).</p> <p>Optimal transport can be understood as a canonical way to lift a ground distance between points to a distance between historgram or measures. In order to evaluate the goodness of mapped paths for optimal transportation choices, the Wasserstein distance was developed.</p> <p>To give a clearer idea of what the Wasserstein distance does, suppose the three functions f,g, and h are in the figure below. If you were to represent the distance between f and g and the distance between f and h with L-infinity norm, you would find that the distance between f and g and between f and h are almost the same (\(\left \| f-g \right \|_{\infty } \approx \left \| f-h \right \|_{\infty }\)). However, under the Wasserstein distance, the distance between f and h is much larger than between f and g (\(d_{w_1}(f,q) &lt; d_{w_1} (f,h)\)).</p> <p>The \(f, g, h\) functions on \(R\):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/fgh-480.webp 480w,/assets/img/optimalTransport/fgh-800.webp 800w,/assets/img/optimalTransport/fgh-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/fgh.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>You can also see the difference between L-infinity norm and Wasserstein distance from the geodesics in the space of functions. If I want to move the function f to the function h, the changes of geodesics in figures below are totally different.</p> <p>The geodesics under L-infinity norm:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/normGeodesics-480.webp 480w,/assets/img/optimalTransport/normGeodesics-800.webp 800w,/assets/img/optimalTransport/normGeodesics-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/normGeodesics.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The geodesics under Wasserstein distance:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/wGeodesics-480.webp 480w,/assets/img/optimalTransport/wGeodesics-800.webp 800w,/assets/img/optimalTransport/wGeodesics-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/wGeodesics.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If we define Wasserstein distance based on the space of probability measures on \(X \subset \mathbb{R}^d\) with bounded \(p^{th}\) moment, i.e.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_11-480.webp 480w,/assets/img/optimalTransport/equation/1_11-800.webp 800w,/assets/img/optimalTransport/equation/1_11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If \(X\) is bounded then \(\mathcal{P}_p(X)=\mathcal{P}(X)\). The Wasserstein distance can be define as</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/1_12-480.webp 480w,/assets/img/optimalTransport/equation/1_12-800.webp 800w,/assets/img/optimalTransport/equation/1_12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/1_12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And the wasserstein distance also satisfies the positive definiteness and triangular inequalities of the fugitive space.</p> <h2 id="duality">Duality</h2> <p>We saw in the previous chapter how Kantorovich’s optimal transport problem resembles a linear programme. It should not therefore be surprising that Kantorovich’s optimal transport problem admits a dual formulation.</p> <p>In optimization theory, turn original optimizing problem into dual problem can easily make it more easier to solve. Regardless of the difficulty of the original problem, dual problems are convex, and convex problems are a class of problems that are relatively easy to solve. When the original problem is a particularly difficult one, it is relatively easy to solve by reducing it to a dyadic problem</p> <h3 id="reminder-of-dual-problem">Reminder of Dual Problem</h3> <p>All optimization problems, in theory, can be transformed into standard form:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/dual1-480.webp 480w,/assets/img/optimalTransport/equation/dual1-800.webp 800w,/assets/img/optimalTransport/equation/dual1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/dual1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And then we define Lagrangian function:</p> \[L(x, \lambda, \mu) = f_0(x) + \sum_{i=1}^{m} \lambda_i f_i(x) + \sum_{j=1}^{p} \mu_j h_j(x)\] <p>With the Lagrangian function, we introduce the Lagrange dual function:</p> \[g(\lambda, \mu) = \inf_{x \in \mathcal{D}} L(x, \lambda, \mu)\] <p>The Lagrange dual function is the Lagrangian function that minimizes with respect to x. And we get the (Lagrangian) dual problem from the standard problem:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/dual2-480.webp 480w,/assets/img/optimalTransport/equation/dual2-800.webp 800w,/assets/img/optimalTransport/equation/dual2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/dual2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="kantorovich-duality">Kantorovich Duality</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/2_1-480.webp 480w,/assets/img/optimalTransport/equation/2_1-800.webp 800w,/assets/img/optimalTransport/equation/2_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/2_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Let \(\Phi_c\) defined by</p> \[\Phi_c = \{(\varphi, \psi) \in L^1(\mu) \times L^1(\nu) : \varphi(x) + \psi(y) \leq c(x, y)\}\] <p>where the inequality is understood to hold for \(\mu\)-almost every \(x \in X\) and \(\nu\)-almost every \(y \in Y\). Then,</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/2_2-480.webp 480w,/assets/img/optimalTransport/equation/2_2-800.webp 800w,/assets/img/optimalTransport/equation/2_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/2_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And if we look at the discrete optimal transport problem,</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/dual3-480.webp 480w,/assets/img/optimalTransport/equation/dual3-800.webp 800w,/assets/img/optimalTransport/equation/dual3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/dual3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>its dual problem is</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimalTransport/equation/2_3-480.webp 480w,/assets/img/optimalTransport/equation/2_3-800.webp 800w,/assets/img/optimalTransport/equation/2_3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/optimalTransport/equation/2_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="optimal-transport"/><category term="math"/><category term="optimalTransport"/><summary type="html"><![CDATA[this is an introduction about Optimal Transport]]></summary></entry></feed>